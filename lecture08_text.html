
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Hypothesis Testing &#8212; 计量经济学讲义</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">计量经济学讲义</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    前言
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture1-CN.html">
   1. 概率论
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-CN.html">
   2. 投影
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-CN.html">
   3. 最小二乘法：线性代数观点
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-CN.html">
   4. 最小二乘: 有限样本理论
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-CN.html">
   5. 基本渐近理论
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/zhentaoshi/metrics1-cn"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/zhentaoshi/metrics1-cn/issues/new?title=Issue%20on%20page%20%2Flecture08_text.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/lecture08_text.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#testing">
   Testing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-rule-and-errors">
     Decision Rule and Errors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimality">
     Optimality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-ratio-test-and-wilks-theorem">
     Likelihood-Ratio Test and Wilks’ theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#score-test">
     Score Test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#confidence-interval-span-id-confidence-interval-label-confidence-interval-confidence-interval-span">
   Confidence Interval
   <span id="confidence-interval" label="confidence-interval">
    [confidence-interval]
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-credible-set">
   Bayesian Credible Set
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-in-ols">
   Applications in OLS
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wald-test">
     Wald Test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lagrangian-multiplier-test">
     Lagrangian Multiplier Test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-ratio-test-for-regression">
     Likelihood-Ratio Test for Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Hypothesis Testing</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#testing">
   Testing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-rule-and-errors">
     Decision Rule and Errors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimality">
     Optimality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-ratio-test-and-wilks-theorem">
     Likelihood-Ratio Test and Wilks’ theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#score-test">
     Score Test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#confidence-interval-span-id-confidence-interval-label-confidence-interval-confidence-interval-span">
   Confidence Interval
   <span id="confidence-interval" label="confidence-interval">
    [confidence-interval]
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-credible-set">
   Bayesian Credible Set
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-in-ols">
   Applications in OLS
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wald-test">
     Wald Test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lagrangian-multiplier-test">
     Lagrangian Multiplier Test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-ratio-test-for-regression">
     Likelihood-Ratio Test for Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="hypothesis-testing">
<h1>Hypothesis Testing<a class="headerlink" href="#hypothesis-testing" title="Permalink to this headline">#</a></h1>
<p>Notation: <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> denotes a random variable or random vector.
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is its realization.</p>
<p>A <em>hypothesis</em> is a statement about the parameter space <span class="math notranslate nohighlight">\(\Theta\)</span>.
Hypothesis testing checks whether the data support a <em>null hypothesis</em>
<span class="math notranslate nohighlight">\(\Theta_{0}\)</span>, which is a subset of <span class="math notranslate nohighlight">\(\Theta\)</span> of interest. Ideally the
null hypothesis should be suggested by scientific theory. The
<em>alternative hypothesis</em> <span class="math notranslate nohighlight">\(\Theta_{1}=\Theta\backslash\Theta_{0}\)</span> is the
complement of <span class="math notranslate nohighlight">\(\Theta_{0}\)</span>. Based on the observed evidence, hypothesis
testing decides to accept or reject the null hypothesis. If the null
hypothesis is rejected by the data, it implies that from the statistical
perspective the data is incompatible with the proposed scientific
theory.</p>
<p>In this chapter, we will first introduce the idea and practice of
hypothesis testing and the related confidence interval. While we mainly
focus on the frequentist interpretation of hypothesis, we briefly
discuss the Bayesian approach to statistical decision. As an application
of the testing procedures to the linear regression model, we elaborate
how to test a linear or nonlinear hypothesis of the slope coefficients
based on the unrestricted or restricted OLS estimators.</p>
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">#</a></h2>
<section id="decision-rule-and-errors">
<h3>Decision Rule and Errors<a class="headerlink" href="#decision-rule-and-errors" title="Permalink to this headline">#</a></h3>
<p>If <span class="math notranslate nohighlight">\(\Theta_{0}\)</span> is a singleton, we call it a <em>simple hypothesis</em>;
otherwise we call it a <em>composite hypothesis</em>. For example, if the
parameter space <span class="math notranslate nohighlight">\(\Theta=\mathbb{R}\)</span>, then <span class="math notranslate nohighlight">\(\Theta_{0}=\left\{ 0\right\}\)</span>
(or equivalently <span class="math notranslate nohighlight">\(\theta_{0}=0\)</span>) is a simple hypothesis, whereas
<span class="math notranslate nohighlight">\(\Theta_{0}=(-\infty,0]\)</span> (or equivalently <span class="math notranslate nohighlight">\(\theta_{0}\leq0\)</span>) is a
composite hypothesis.</p>
<p>A <em>test function</em> is a mapping
$<span class="math notranslate nohighlight">\(\phi:\mathcal{X}^{n}\mapsto\left\{ 0,1\right\} ,\)</span><span class="math notranslate nohighlight">\( where \)</span>\mathcal{X}<span class="math notranslate nohighlight">\(
is the sample space. The null hypothesis is accepted if
\)</span>\phi\left(\mathbf{x}\right)=0<span class="math notranslate nohighlight">\(, or rejected if
\)</span>\phi\left(\mathbf{x}\right)=1<span class="math notranslate nohighlight">\(. We call the set
\)</span>A_{\phi}=\left{ \mathbf{x}\in\mathcal{X}^{n}:\phi_{\theta}\left(\mathbf{x}\right)=0\right}<span class="math notranslate nohighlight">\(
the *acceptance region*, and its complement
\)</span>R_{\phi}=\left{ \mathbf{x}\in\mathcal{X}^{n}:\phi\left(\mathbf{x}\right)=1\right}$
the <em>rejection region.</em></p>
<p>The <em>power function</em> of a test <span class="math notranslate nohighlight">\(\phi\)</span> is
$<span class="math notranslate nohighlight">\(\beta\left(\theta\right)=P_{\theta}\left\{ \phi\left(\mathbf{X}\right)=1\right\} =E_{\theta}\left[\phi\left(\mathbf{X}\right)\right].\)</span><span class="math notranslate nohighlight">\(
The power function measures the probability that the test function
rejects the null when the data is generated under the true parameter
\)</span>\theta<span class="math notranslate nohighlight">\(, reflected in \)</span>P_{\theta}<span class="math notranslate nohighlight">\( and \)</span>E_{\theta}$.</p>
<p>The <em>power</em> of a test for some <span class="math notranslate nohighlight">\(\theta\in\Theta_{1}\)</span> is the value of
<span class="math notranslate nohighlight">\(\beta\left(\theta\right)\)</span>. The <em>size</em> of the test is
<span class="math notranslate nohighlight">\(\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right).\)</span> Notice that the
definition of power depends on a <span class="math notranslate nohighlight">\(\theta\)</span> in the alternative hypothesis
<span class="math notranslate nohighlight">\(\Theta_{1}\)</span>, whereas that of size is independent of <span class="math notranslate nohighlight">\(\theta\)</span> due to the
supremum over the set of null <span class="math notranslate nohighlight">\(\Theta_{0}\)</span>. The <em>level</em> of a test is any
value <span class="math notranslate nohighlight">\(\alpha\in\left(0,1\right)\)</span> such that
<span class="math notranslate nohighlight">\(\alpha\geq\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\)</span>, which is
often used when it is difficult to attain the exact supremum. A test of
size <span class="math notranslate nohighlight">\(\alpha\)</span> is also of level <span class="math notranslate nohighlight">\(\alpha\)</span> or bigger; while a test of level
<span class="math notranslate nohighlight">\(\alpha\)</span> must have size smaller or equal to <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>The concept of <em>level</em> is useful if we do not have sufficient
information to derive the exact size of a test. If
<span class="math notranslate nohighlight">\(\left(X_{1i},X_{2i}\right)_{i=1}^{n}\)</span> are randomly drawn from some
unknown joint distribution, but we know the marginal distribution is
<span class="math notranslate nohighlight">\(X_{ji}\sim N\left(\theta_{j},1\right)\)</span>, for <span class="math notranslate nohighlight">\(j=1,2\)</span>. In order to test
the joint hypothesis <span class="math notranslate nohighlight">\(\theta_{1}=\theta_{2}=0\)</span>, we can construct a test
function
$<span class="math notranslate nohighlight">\(\phi_{\theta_{1}=\theta_{2}=0}\left(\mathbf{X}_{1},\mathbf{X}_{2}\right)=1\left\{ \left\{ \sqrt{n}\left|\overline{X}_{1}\right|\geq z_{1-\alpha/4}\right\} \cup\left\{ \sqrt{n}\left|\overline{X}_{2}\right|\geq z_{1-\alpha/4}\right\} \right\} ,\)</span><span class="math notranslate nohighlight">\(
where \)</span>z_{1-\alpha/4}<span class="math notranslate nohighlight">\( is the \)</span>\left(1-\alpha/4\right)<span class="math notranslate nohighlight">\(-th quantile of
the standard normal distribution. The level of this test is
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}P\left(\phi_{\theta_{1}=\theta_{2}=0}\left(\mathbf{X}_{1},\mathbf{X}_{2}\right)\right) &amp; \leq P\left(\sqrt{n}\left|\overline{X}_{1}\right|\geq z_{1-\alpha/4}\right)+P\left(\sqrt{n}\left|\overline{X}_{2}\right|\geq z_{1-\alpha/4}\right)\\
 &amp; =\alpha/2+\alpha/2=\alpha.
\end{aligned}\)</span><span class="math notranslate nohighlight">\( where the inequality follows by the *Bonferroni
inequality*
\)</span><span class="math notranslate nohighlight">\(P\left(A\cup B\right)\leq P\left(A\right)+P\left(B\right).\)</span><span class="math notranslate nohighlight">\( (The
seemingly trivial Bonferroni inequality is useful in many proofs of
probability results.) Therefore, the level of
\)</span>\phi\left(\mathbf{X}<em>{1},\mathbf{X}</em>{2}\right)<span class="math notranslate nohighlight">\( is \)</span>\alpha<span class="math notranslate nohighlight">\(, but the
exact size is unknown without the knowledge of the joint distribution.
(Even if we know the correlation of \)</span>X_{1i}<span class="math notranslate nohighlight">\( and \)</span>X_{2i}$, putting two
marginally normal distributions together does not make a jointly normal
vector in general.)</p>
<hr class="docutils" />
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>                   accept $H_{0}$     reject $H_{0}$
  $H_{0}$ true    correct decision     Type I error
  $H_{0}$ false    Type II error     correct decision
</pre></div>
</div>
<hr class="docutils" />
<p>: <span id="tab:Decisions-and-States"
label="tab:Decisions-and-States">[tab:Decisions-and-States]</span>
Actions, States and Consequences</p>
<ul class="simple">
<li><p>The <em>probability of committing Type I error</em> is
<span class="math notranslate nohighlight">\(\beta\left(\theta\right)\)</span> for some <span class="math notranslate nohighlight">\(\theta\in\Theta_{0}\)</span>.</p></li>
<li><p>The <em>probability of committing Type II error</em> is
<span class="math notranslate nohighlight">\(1-\beta\left(\theta\right)\)</span> for some <span class="math notranslate nohighlight">\(\theta\in\Theta_{1}\)</span>.</p></li>
</ul>
<p>The philosophy on hypothesis testing has been debated for centuries. At
present the prevailing framework in statistics textbooks is the
<em>frequentist perspective</em>. A frequentist views the parameter as a fixed
constant. They keep a conservative attitude about the Type I error: Only
if overwhelming evidence is demonstrated shall a researcher reject the
null. Under the principle of protecting the null hypothesis, a desirable
test should have a small level. Conventionally we take <span class="math notranslate nohighlight">\(\alpha=0.01,\)</span>
0.05 or 0.1. We say a test is <em>unbiased</em> if
<span class="math notranslate nohighlight">\(\beta\left(\theta\right)&gt;\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\)</span>
for all <span class="math notranslate nohighlight">\(\theta\in\Theta_{1}\)</span>. There can be many tests of correct size.</p>
<p>A trivial test function
<span class="math notranslate nohighlight">\(\phi(\mathbf{x})=1\left\{ 0\leq U\leq\alpha\right\}\)</span> for all
<span class="math notranslate nohighlight">\(\theta\in\Theta\)</span>, where <span class="math notranslate nohighlight">\(U\)</span> is a random variable from a uniform
distribution on <span class="math notranslate nohighlight">\(\left[0,1\right]\)</span>, has correct size <span class="math notranslate nohighlight">\(\alpha\)</span> but no
non-trivial power at the alternative. On the other extreme, the trivial
test function <span class="math notranslate nohighlight">\(\phi\left(\mathbf{x}\right)=1\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
enjoys the biggest power but suffers incorrect size.</p>
<p>Usually, we design a test by proposing a test statistic
<span class="math notranslate nohighlight">\(T_{n}:\mathcal{X}^{n}\mapsto\mathbb{R}^{+}\)</span> and a critical value
<span class="math notranslate nohighlight">\(c_{1-\alpha}\)</span>. Given <span class="math notranslate nohighlight">\(T_{n}\)</span> and <span class="math notranslate nohighlight">\(c_{1-\alpha}\)</span>, we write the test
function as
$<span class="math notranslate nohighlight">\(\phi\left(\mathbf{X}\right)=1\left\{ T_{n}\left(\mathbf{X}\right)&gt;c_{1-\alpha}\right\} .\)</span><span class="math notranslate nohighlight">\(
To ensure such a \)</span>\phi\left(\mathbf{x}\right)<span class="math notranslate nohighlight">\( has correct size, we need
to figure out the distribution of \)</span>T_{n}<span class="math notranslate nohighlight">\( under the null hypothesis
(called the *null distribution*), and choose a critical value
\)</span>c_{1-\alpha}<span class="math notranslate nohighlight">\( according to the null distribution and the desirable size
or level \)</span>\alpha$.</p>
<p>Another commonly used indicator in hypothesis testing is <span class="math notranslate nohighlight">\(p\)</span>-value:
$<span class="math notranslate nohighlight">\(\sup_{\theta\in\Theta_{0}}P_{\theta}\left\{ T_{n}\left(\mathbf{x}\right)\leq T_{n}\left(\mathbf{X}\right)\right\} .\)</span><span class="math notranslate nohighlight">\(
In the above expression, \)</span>T_{n}\left(\mathbf{x}\right)<span class="math notranslate nohighlight">\( is the realized
value of the test statistic \)</span>T_{n}<span class="math notranslate nohighlight">\(, while
\)</span>T_{n}\left(\mathbf{X}\right)<span class="math notranslate nohighlight">\( is the random variable generated by
\)</span>\mathbf{X}<span class="math notranslate nohighlight">\( under the null \)</span>\theta\in\Theta_{0}<span class="math notranslate nohighlight">\(. The interpretation of
the \)</span>p<span class="math notranslate nohighlight">\(-value is tricky. \)</span>p<span class="math notranslate nohighlight">\(-value is the probability that we observe
\)</span>T_{n}(\mathbf{X})<span class="math notranslate nohighlight">\( being greater than the realized \)</span>T_{n}(\mathbf{x})$
if the null hypothesis is true.</p>
<p><span class="math notranslate nohighlight">\(p\)</span>-value is <em>not</em> the probability that the null hypothesis is true.
Under the frequentist perspective, the null hypothesis is either true or
false, with certainty. The randomness of a test comes only from
sampling, not from the hypothesis. <span class="math notranslate nohighlight">\(p\)</span>-value measures whether the
dataset is compatible with the null hypothesis. <span class="math notranslate nohighlight">\(p\)</span>-value is closely
related to the corresponding test. When <span class="math notranslate nohighlight">\(p\)</span>-value is smaller than the
specified test size <span class="math notranslate nohighlight">\(\alpha\)</span>, the test rejects the null.</p>
<p>So far we have been talking about hypothesis testing in finite sample.
The discussion and terminologies can be carried over to the asymptotic
world when <span class="math notranslate nohighlight">\(n\to\infty\)</span>. If we denote the power function as
<span class="math notranslate nohighlight">\(\beta_{n}\left(\theta\right)\)</span>, in which we make its dependence on the
sample size <span class="math notranslate nohighlight">\(n\)</span> explicit, the test is of asymptotic size <span class="math notranslate nohighlight">\(\alpha\)</span> if
<span class="math notranslate nohighlight">\(\limsup_{n\to\infty}\beta_{n}\left(\theta\right)\leq\alpha\)</span> for all
<span class="math notranslate nohighlight">\(\theta\in\Theta_{0}\)</span>. A test is <em>consistent</em> if
<span class="math notranslate nohighlight">\(\beta_{n}\left(\theta\right)\to1\)</span> for every <span class="math notranslate nohighlight">\(\theta\in\Theta_{1}\)</span>.</p>
</section>
<section id="optimality">
<h3>Optimality<a class="headerlink" href="#optimality" title="Permalink to this headline">#</a></h3>
<p>Just as there may be multiple valid estimators for a task of estimation,
there may be multiple tests for a task of hypothesis testing. For a
class of tests of the same level <span class="math notranslate nohighlight">\(\alpha\)</span> under the null
<span class="math notranslate nohighlight">\(\Psi_{\alpha}=\left\{ \phi:\sup_{\theta\in\Theta_{0}}\beta_{\phi}\left(\theta\right)\leq\alpha\right\}\)</span>
where we put a subscript <span class="math notranslate nohighlight">\(\phi\)</span> in <span class="math notranslate nohighlight">\(\beta_{\phi}\left(\theta\right)\)</span> to
distinguish the power for different tests, it is natural to prefer a
test <span class="math notranslate nohighlight">\(\phi^{*}\)</span> that exhibits higher power than all other tests under
consideration at each point of the alternative hypothesis in that
$<span class="math notranslate nohighlight">\(\beta_{\phi^{*}}\left(\theta\right)\geq\beta_{\phi}\left(\theta\right)\)</span><span class="math notranslate nohighlight">\(
for every \)</span>\theta\in\Theta_{1}<span class="math notranslate nohighlight">\( and every \)</span>\phi\in\Psi_{\alpha}<span class="math notranslate nohighlight">\(. If
such a test \)</span>\phi^{*}\in\Psi_{\alpha}$ exists, we call it the <em>uniformly
most powerful test.</em></p>
<p>Suppose a random sample of size 6 is generated from
$<span class="math notranslate nohighlight">\(\left(X_{1},\ldots,X_{6}\right)\sim\text{iid.}N\left(\theta,1\right),\)</span><span class="math notranslate nohighlight">\(
where \)</span>\theta<span class="math notranslate nohighlight">\( is unknown. We want to infer the population mean of the
normal distribution. The null hypothesis is \)</span>H_{0}<span class="math notranslate nohighlight">\(: \)</span>\theta\leq0<span class="math notranslate nohighlight">\( and
the alternative is \)</span>H_{1}<span class="math notranslate nohighlight">\(: \)</span>\theta&gt;0<span class="math notranslate nohighlight">\(. All tests in
\)</span><span class="math notranslate nohighlight">\(\Psi=\left\{ 1\left\{ \bar{X}\geq c/\sqrt{6}\right\} :c\geq1.64\right\}\)</span><span class="math notranslate nohighlight">\(
has the correct level. Since \)</span>\bar{X}=N\left(\theta,1/6\right)<span class="math notranslate nohighlight">\(, the
power function for those in \)</span>\Psi<span class="math notranslate nohighlight">\( is \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\beta_{\phi}\left(\theta\right) &amp; =P\left(\bar{X}\geq\frac{c}{\sqrt{6}}\right)=P\left(\frac{\bar{X}-\theta}{1/\sqrt{6}}\geq\frac{\frac{c}{\sqrt{6}}-\theta}{1/\sqrt{6}}\right)\\
 &amp; =P\left(N\geq c-\sqrt{6}\theta\right)=1-\Phi\left(c-\sqrt{6}\theta\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where \)</span>N=\frac{\bar{X}-\theta}{1/\sqrt{6}}<span class="math notranslate nohighlight">\( follows the standard normal,
and \)</span>\Phi<span class="math notranslate nohighlight">\( is the cdf of the standard normal. It is clear that
\)</span>\beta_{\phi}\left(\theta\right)<span class="math notranslate nohighlight">\( is monotonically decreasing in \)</span>c<span class="math notranslate nohighlight">\(.
Thus the test function
\)</span><span class="math notranslate nohighlight">\(\phi_{\theta=0}\left(\mathbf{X}\right)=1\left\{ \bar{X}\geq1.64/\sqrt{6}\right\}\)</span><span class="math notranslate nohighlight">\(
is the most powerful test in \)</span>\Psi<span class="math notranslate nohighlight">\(, as \)</span>c=1.64<span class="math notranslate nohighlight">\( is the lower bound that
\)</span>\Psi_{\alpha}<span class="math notranslate nohighlight">\( allows in order to keep the level \)</span>\alpha$.</p>
</section>
<section id="likelihood-ratio-test-and-wilks-theorem">
<h3>Likelihood-Ratio Test and Wilks’ theorem<a class="headerlink" href="#likelihood-ratio-test-and-wilks-theorem" title="Permalink to this headline">#</a></h3>
<p>When estimators are not available in closed-forms, the likelihood-ratio
test (LRT) serves as a very general testing statistic under the
likelihood principle. Let
<span class="math notranslate nohighlight">\(\ell_{n}\left(\theta\right)=n^{-1}\sum_{i}\log f\left(x_{i};\theta\right)\)</span>
be the average sample log-likelihood, and
<span class="math notranslate nohighlight">\(\widehat{\theta}=\arg\max_{\theta\in\Theta}\ell_{n}\left(\theta\right)\)</span>
is the maximum likelihood estimator (MLE). Take a Taylor expansion of
<span class="math notranslate nohighlight">\(\ell_{n}\left(\theta_{0}\right)\)</span> around
<span class="math notranslate nohighlight">\(\ell_{n}\left(\widehat{\theta}\right)\)</span>: $<span class="math notranslate nohighlight">\(\begin{aligned}
\ell_{n}\left(\theta_{0}\right)-\ell_{n}\left(\widehat{\theta}\right) &amp; =\frac{\partial\ell_{n}}{\partial\theta}\left(\widehat{\theta}\right)'\left(\theta_{0}-\widehat{\theta}\right)+\frac{1}{2}\left(\theta_{0}-\widehat{\theta}\right)'\left(\frac{\partial^{2}}{\partial\theta\partial\theta'}\ell_{n}\left(\theta_{0}\right)\right)\left(\theta_{0}-\widehat{\theta}\right)+O\left(\left\Vert \widehat{\theta}-\theta_{0}\right\Vert _{2}^{3}\right)\\
 &amp; =\frac{1}{2}\left(\widehat{\theta}-\theta_{0}\right)'\left(\frac{\partial^{2}}{\partial\theta\partial\theta'}\ell_{n}\left(\theta_{0}\right)\right)\left(\widehat{\theta}-\theta_{0}\right)+O\left(\left\Vert \widehat{\theta}-\theta_{0}\right\Vert _{2}^{3}\right)\\
 &amp; =\frac{1}{2}\left(\widehat{\theta}-\theta_{0}\right)'\left(\frac{\partial^{2}}{\partial\theta\partial\theta'}\ell_{n}\left(\theta_{0}\right)\right)\left(\widehat{\theta}-\theta_{0}\right)+o_{p}\left(1\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
by that
\)</span>\frac{\partial\ell_{n}}{\partial\theta}\left(\widehat{\theta}\right)=0<span class="math notranslate nohighlight">\(
due to the first order condition of optimality. Define
\)</span>L_{n}\left(\theta\right):=\sum_{i}\log f\left(x_{i};\theta\right)<span class="math notranslate nohighlight">\(, and
the *likelihood-ratio statistic* as
\)</span><span class="math notranslate nohighlight">\(\mathcal{LR}:=2\left(L_{n}\left(\widehat{\theta}\right)-L_{n}\left(\theta_{0}\right)\right)=2n\left(\ell_{n}\left(\widehat{\theta}\right)-\ell_{n}\left(\theta_{0}\right)\right).\)</span><span class="math notranslate nohighlight">\(
Obviously \)</span>\mathcal{LR}\geq0<span class="math notranslate nohighlight">\( because \)</span>\widehat{\theta}<span class="math notranslate nohighlight">\( maximizes
\)</span>\ell_{n}\left(\theta\right)<span class="math notranslate nohighlight">\(. Multiply \)</span>-2n<span class="math notranslate nohighlight">\( to the two sides of the
above Taylor expansion:
\)</span><span class="math notranslate nohighlight">\(\mathcal{LR}=\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)'\left(-\frac{\partial^{2}}{\partial\theta\partial\theta'}\ell_{n}\left(\dot{\theta}\right)\right)\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)+o_{p}\left(1\right)\)</span><span class="math notranslate nohighlight">\(
Notice that when the model is correctly specified we have
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
-\frac{\partial^{2}}{\partial\theta\partial\theta'}\ell_{n}\left(\theta_{0}\right) &amp; \stackrel{p}{\to}-\mathcal{H}\left(\theta_{0}\right)=\mathcal{I}\left(\theta_{0}\right)\\
\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right) &amp; \stackrel{d}{\to}N\left(0,\mathcal{I}^{-1}\left(\theta_{0}\right)\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
By Slutsky’s theorem:
\)</span><span class="math notranslate nohighlight">\(\left(-\frac{\partial^{2}}{\partial\theta\partial\theta'}\ell_{n}\left(\dot{\theta}\right)\right)^{1/2}\left[\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\right]\stackrel{d}{\to}\mathcal{I}^{1/2}\left(\theta_{0}\right)\times N\left(0,\mathcal{I}^{-1}\left(\theta_{0}\right)\right)\sim N\left(0,I_{k}\right).\)</span><span class="math notranslate nohighlight">\(
and then \)</span>\mathcal{LR}\stackrel{d}{\to}\chi_{K}^{2}$ by the continuous
mapping theorem.</p>
<p><em>Wilks’ theorem</em>, or <em>Wilks’ phenomenon</em> is referred to the fact that
<span class="math notranslate nohighlight">\(\mathcal{LR}\stackrel{d}{\to}\chi^{2}\left(K\right)\)</span> when the
parametric model is correctly specified.</p>
</section>
<section id="score-test">
<h3>Score Test<a class="headerlink" href="#score-test" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="confidence-interval-span-id-confidence-interval-label-confidence-interval-confidence-interval-span">
<h2>Confidence Interval<span id="confidence-interval" label="confidence-interval">[confidence-interval]</span><a class="headerlink" href="#confidence-interval-span-id-confidence-interval-label-confidence-interval-confidence-interval-span" title="Permalink to this headline">#</a></h2>
<p>An <em>interval estimate</em> is a function
<span class="math notranslate nohighlight">\(C:\mathcal{X}^{n}\mapsto\left\{ \Theta_{1}:\Theta_{1}\subseteq\Theta\right\}\)</span>
that maps a point in the sample space to a subset of the parameter
space. The <em>coverage probability</em> of an <em>interval estimator</em>
<span class="math notranslate nohighlight">\(C\left(\mathbf{X}\right)\)</span> is defined as
<span class="math notranslate nohighlight">\(P_{\theta}\left(\theta\in C\left(\mathbf{X}\right)\right)\)</span>. When
<span class="math notranslate nohighlight">\(\theta\)</span> is of one dimension, we usually call the interval estimator
<em>confidence interval</em>. When <span class="math notranslate nohighlight">\(\theta\)</span> is of multiple dimensions, we call
the it <em>confidence region</em> and it of course includes the one-dimensional
<span class="math notranslate nohighlight">\(\theta\)</span> as a special case. The coverage probability is the frequency
that the interval estimator captures the true parameter that generates
the sample. From the frequentist perspective, the parameter is fixed
while the confidence region is random. It is <em>not</em> the probability that
<span class="math notranslate nohighlight">\(\theta\)</span> is inside the given confidence interval.</p>
<p>Suppose a random sample of size 6 is generated from
<span class="math notranslate nohighlight">\(\left(X_{1},\ldots,X_{6}\right)\sim\text{iid }N\left(\theta,1\right).\)</span>
Find the coverage probability of the random interval is
<span class="math notranslate nohighlight">\(\left[\bar{X}-1.96/\sqrt{6},\ \bar{X}+1.96/\sqrt{6}\right].\)</span></p>
<p>Hypothesis testing and confidence region are closely related. Sometimes
it is difficult to directly construct the confidence region, but easy to
test a hypothesis. One way to construct confidence region is by
<em>inverting a test</em>. Suppose <span class="math notranslate nohighlight">\(\phi_{\theta}\)</span> is a test of size <span class="math notranslate nohighlight">\(\alpha\)</span>.
If <span class="math notranslate nohighlight">\(C\left(\mathbf{X}\right)\)</span> is constructed as
$<span class="math notranslate nohighlight">\(C\left(\mathbf{X}\right)=\left\{ \theta\in\Theta:\phi\left(\mathbf{X}\right)=0\right\} .\)</span><span class="math notranslate nohighlight">\(
The coverage probability of the true data generating parameter \)</span>\theta<span class="math notranslate nohighlight">\(
is
\)</span><span class="math notranslate nohighlight">\(P_{\theta}\left\{ \theta\in C\left(\mathbf{X}\right)\right\} =P_{\theta}\left\{ \phi\left(\mathbf{X}\right)=0\right\} =1-P_{\theta}\left\{ \phi\left(\mathbf{X}\right)=1\right\} =1-\beta\left(\theta\right)\geq1-\alpha\)</span><span class="math notranslate nohighlight">\(
where the last inequality follows as
\)</span>\beta\left(\theta\right)\leq\alpha<span class="math notranslate nohighlight">\( for \)</span>\theta\in\Theta_{0}<span class="math notranslate nohighlight">\(. If
\)</span>\Theta_{0}$ is a singleton, the equality holds.</p>
<p><strong>knitr</strong></p>
</section>
<section id="bayesian-credible-set">
<h2>Bayesian Credible Set<a class="headerlink" href="#bayesian-credible-set" title="Permalink to this headline">#</a></h2>
<p>The Bayesian framework offers a coherent and natural language for
statistical decision. However, the major criticism against Bayesian
statistics is the arbitrariness of the choice of the prior.</p>
<p>The Bayesian approach views both the data <span class="math notranslate nohighlight">\(\mathbf{X}_{n}\)</span> and the
parameter <span class="math notranslate nohighlight">\(\theta\)</span> as random variables. Before she observes the data,
she holds a <em>prior distribution</em> <span class="math notranslate nohighlight">\(\pi\)</span> about <span class="math notranslate nohighlight">\(\theta\)</span>. After observing
the data, she updates the prior distribution to a <em>posterior
distribution</em> <span class="math notranslate nohighlight">\(p(\theta|\mathbf{X}_{n})\)</span>. The <em>Bayes Theorem</em> connects
the prior and the posterior as
$<span class="math notranslate nohighlight">\(p(\theta|\mathbf{X}_{n})\propto f(\mathbf{X}_{n}|\theta)\pi(\theta)\)</span><span class="math notranslate nohighlight">\(
where \)</span>f(\mathbf{X}_{n}|\theta)$ is the likelihood function.</p>
<p>Here is a classical example to illustrate the Bayesian approach to
statistical inference. Suppose <span class="math notranslate nohighlight">\(\mathbf{X}_{n}=(X_{1},\ldots,X_{n})\)</span> is
an iid sample drawn from a normal distribution with unknown <span class="math notranslate nohighlight">\(\theta\)</span> and
known <span class="math notranslate nohighlight">\(\sigma\)</span>. If a researcher’s prior distribution
<span class="math notranslate nohighlight">\(\theta\sim N(\theta_{0},\sigma_{0}^{2})\)</span>, her posterior distribution
is, by some routine calculation, also a normal distribution
$<span class="math notranslate nohighlight">\(p(\theta|\mathbf{x}_{n})\sim N\left(\tilde{\theta},\tilde{\sigma}^{2}\right),\)</span><span class="math notranslate nohighlight">\(
where
\)</span>\tilde{\theta}=\frac{\sigma^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\theta_{0}+\frac{n\sigma_{0}^{2}}{n\sigma_{0}^{2}+\sigma^{2}}\bar{x}<span class="math notranslate nohighlight">\(
and
\)</span>\tilde{\sigma}^{2}=\frac{\sigma_{0}^{2}\sigma^{2}}{n\sigma_{0}^{2}+\sigma^{2}}<span class="math notranslate nohighlight">\(.
Thus the Bayesian credible set is
\)</span><span class="math notranslate nohighlight">\(\left(\tilde{\theta}-z_{1-\alpha/2}\cdot\tilde{\sigma},\ \tilde{\theta}+z_{1-\alpha/2}\cdot\tilde{\sigma}\right).\)</span><span class="math notranslate nohighlight">\(
This posterior distribution depends on \)</span>\theta_{0}<span class="math notranslate nohighlight">\( and \)</span>\sigma_{0}^{2}<span class="math notranslate nohighlight">\(
from the prior. When the sample size is sufficiently large the posterior
can be approximated by \)</span>N(\bar{x},\sigma^{2}/n)$, where the prior
information is overwhelmed by the information accumulated from the data.</p>
<p>In contrast, a frequentist will estimate
<span class="math notranslate nohighlight">\(\hat{\theta}=\bar{x}\sim N(\theta,\sigma^{2}/n)\)</span>. Her confidence
interval is
$<span class="math notranslate nohighlight">\(\left(\bar{x}-z_{1-\alpha/2}\cdot\sigma/\sqrt{n},\ \bar{x}-z_{1-\alpha/2}\cdot\sigma/\sqrt{n}\right).\)</span><span class="math notranslate nohighlight">\(
The Bayesian credible set and the frequentist confidence interval are
different for finite \)</span>n<span class="math notranslate nohighlight">\(, but they coincide when \)</span>n\to\infty$.</p>
</section>
<section id="applications-in-ols">
<h2>Applications in OLS<a class="headerlink" href="#applications-in-ols" title="Permalink to this headline">#</a></h2>
<p>We will introduce three tests for a hypothesis of the linear regression
coefficients, namely the Wald test, the Lagrangian multiplier (LM) test,
and the likelihood ratio test. The Wald test is based on the
unrestricted OLS estimator <span class="math notranslate nohighlight">\(\widehat{\beta}\)</span>. The LM test is based on
the restricted estimator <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span>. The LRT, as we have discussed,
is based on the difference of the log-likelihood function evaluated at
the unrestricted OLS estimator and that on the restricted estimator.</p>
<p>Let <span class="math notranslate nohighlight">\(R\)</span> be a <span class="math notranslate nohighlight">\(q\times K\)</span> constant matrix with <span class="math notranslate nohighlight">\(q\leq K\)</span> and
<span class="math notranslate nohighlight">\(\mbox{rank}\left(R\right)=q\)</span>. All linear restrictions about <span class="math notranslate nohighlight">\(\beta\)</span> can
be written in the form of <span class="math notranslate nohighlight">\(R\beta=r\)</span>, where <span class="math notranslate nohighlight">\(r\)</span> is a <span class="math notranslate nohighlight">\(q\times1\)</span> constant
vector.</p>
<p>We want to simultaneously test <span class="math notranslate nohighlight">\(\beta_{1}=1\)</span> and <span class="math notranslate nohighlight">\(\beta_{3}+\beta_{4}=2\)</span>
in the above example. The null hypothesis can be expressed in the
general form <span class="math notranslate nohighlight">\(R\beta=r\)</span>, where the restriction matrix <span class="math notranslate nohighlight">\(R\)</span> is
$<span class="math notranslate nohighlight">\(R=\begin{pmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 0
\end{pmatrix}\)</span><span class="math notranslate nohighlight">\( and \)</span>r=\left(1,2\right)’$.</p>
<section id="wald-test">
<h3>Wald Test<a class="headerlink" href="#wald-test" title="Permalink to this headline">#</a></h3>
<p>Suppose the OLS estimator <span class="math notranslate nohighlight">\(\widehat{\beta}\)</span> is asymptotic normal, i.e.
$<span class="math notranslate nohighlight">\(\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,\Omega\right)\)</span><span class="math notranslate nohighlight">\(
where \)</span>\Omega<span class="math notranslate nohighlight">\( is a \)</span>K\times K<span class="math notranslate nohighlight">\( positive definite covariance matrix and.
Since
\)</span>R\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,R\Omega R’\right)<span class="math notranslate nohighlight">\(,
the quadratic form
\)</span><span class="math notranslate nohighlight">\(n\left(\widehat{\beta}-\beta\right)'R'\left(R\Omega R'\right)^{-1}R\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}\chi_{q}^{2}.\)</span><span class="math notranslate nohighlight">\(
Now we intend to test the linear null hypothesis \)</span>R\beta=r<span class="math notranslate nohighlight">\(. Under the
null, the Wald statistic
\)</span><span class="math notranslate nohighlight">\(\mathcal{W}=n\left(R\widehat{\beta}-r\right)'\left(R\widehat{\Omega}R'\right)^{-1}\left(R\widehat{\beta}-r\right)\stackrel{d}{\to}\chi_{q}^{2}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\widehat{\Omega}<span class="math notranslate nohighlight">\( is a consistent estimator of \)</span>\Omega$.</p>
<p>(Single test) In a linear regression
$<span class="math notranslate nohighlight">\(\begin{aligned}y &amp; =x_{i}'\beta+e_{i}=\sum_{k=1}^{5}\beta_{k}x_{ik}+e_{i}.\nonumber\\
E\left[e_{i}x_{i}\right] &amp; =\mathbf{0}_{5},\label{eq:example}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( where \)</span>y<span class="math notranslate nohighlight">\( is wage and
\)</span><span class="math notranslate nohighlight">\(x=\left(\mbox{edu},\mbox{age},\mbox{experience},\mbox{experience}^{2},1\right)'.\)</span><span class="math notranslate nohighlight">\(
To test whether *education* affects *wage*, we specify the null
hypothesis \)</span>\beta_{1}=0<span class="math notranslate nohighlight">\(. Let \)</span>R=\left(1,0,0,0,0\right)<span class="math notranslate nohighlight">\( and \)</span>r=0<span class="math notranslate nohighlight">\(.
\)</span><span class="math notranslate nohighlight">\(\sqrt{n}\widehat{\beta}_{1}=\sqrt{n}\left(\widehat{\beta}_{1}-\beta_{1}\right)=\sqrt{n}R\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,R\Omega R'\right)\sim N\left(0,\Omega_{11}\right),\label{eq:R11}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\Omega{}<em>{11}<span class="math notranslate nohighlight">\( is the \)</span>\left(1,1\right)<span class="math notranslate nohighlight">\( (scalar) element of
\)</span>\Omega<span class="math notranslate nohighlight">\(. Under
\)</span><span class="math notranslate nohighlight">\(H_{0}:R\beta=\left(1,0,0,0,0\right)\left(\beta_{1},\ldots,\beta_{5}\right)'=\beta_{1}=0,\)</span><span class="math notranslate nohighlight">\(
we have
\)</span>\sqrt{n}R\left(\widehat{\beta}-\beta\right)=\sqrt{n}\widehat{\beta}</em>{1}\stackrel{d}{\to}N\left(0,\Omega_{11}\right).<span class="math notranslate nohighlight">\(
Therefore,
\)</span><span class="math notranslate nohighlight">\(\sqrt{n}\frac{\widehat{\beta}_{1}}{\widehat{\Omega}_{11}^{1/2}}=\sqrt{\frac{\Omega_{11}}{\widehat{\Omega}_{11}}}\sqrt{n}\frac{\widehat{\beta}_{1}}{\sqrt{\Omega_{11}}}\)</span><span class="math notranslate nohighlight">\(
If \)</span>\widehat{\Omega}\stackrel{p}{\to}\Omega<span class="math notranslate nohighlight">\(, then
\)</span>\left(\Omega_{11}/\widehat{\Omega}<em>{11}\right)^{1/2}\stackrel{p}{\to}1<span class="math notranslate nohighlight">\(
by the continuous mapping theorem. As
\)</span>\sqrt{n}\widehat{\beta}</em>{1}/\Omega_{11}^{1/2}\stackrel{d}{\to}N\left(0,1\right)<span class="math notranslate nohighlight">\(,
we conclude
\)</span>\sqrt{n}\widehat{\beta}<em>{1}/\widehat{\Omega}</em>{11}^{1/2}\stackrel{d}{\to}N\left(0,1\right).$</p>
<p>The above example is a test about a single coefficient, and the test
statistic is essentially the square of the <em>t</em>-statistic, and the null
distribution is the square of a standard normal.</p>
<p>In order to test a nonlinear regression, we use the delta method.</p>
<p><em>(This is not a good example because it can be rewritten into a linear
hypothesis.)</em> In the example of linear regression, the optimal
experience level can be found by setting to zero the first order
condition with respective to experience,
<span class="math notranslate nohighlight">\(\beta_{3}+2\beta_{4}\mbox{experience}^{*}=0\)</span>. We test the hypothesis
that the optimal experience level is 20 years; in other words,
$<span class="math notranslate nohighlight">\(\mbox{experience}^{*}=-\frac{\beta_{3}}{2\beta_{4}}=20.\)</span><span class="math notranslate nohighlight">\( This is a
nonlinear hypothesis. If \)</span>q\leq K<span class="math notranslate nohighlight">\( where \)</span>q<span class="math notranslate nohighlight">\( is the number of
restrictions, we have
\)</span><span class="math notranslate nohighlight">\(n\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)'\left(\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\Omega\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)'\right)^{-1}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}\chi_{q}^{2},\)</span><span class="math notranslate nohighlight">\(
where in this example, \)</span>\theta=\beta<span class="math notranslate nohighlight">\(,
\)</span>f\left(\beta\right)=-\beta_{3}/\left(2\beta_{4}\right)<span class="math notranslate nohighlight">\(. The gradient
\)</span><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial\beta'}\left(\beta\right)=\left(0,0,-\frac{1}{2\beta_{4}},\frac{\beta_{3}}{2\beta_{4}^{2}},0\right)\)</span><span class="math notranslate nohighlight">\(
Since \)</span>\widehat{\beta}\stackrel{p}{\to}\beta_{0}<span class="math notranslate nohighlight">\(, by the continuous
mapping theorem, if \)</span>\beta_{0,4}\neq0<span class="math notranslate nohighlight">\(, we have
\)</span>\frac{\partial}{\partial\beta}f\left(\widehat{\beta}\right)\stackrel{p}{\to}\frac{\partial}{\partial\beta}f\left(\beta_{0}\right)<span class="math notranslate nohighlight">\(.
Therefore, the (nonlinear) Wald test is
\)</span><span class="math notranslate nohighlight">\(\mathcal{W}=n\left(f\left(\widehat{\beta}\right)-20\right)'\left(\frac{\partial f}{\partial\beta'}\left(\widehat{\beta}\right)\widehat{\Omega}\frac{\partial f}{\partial\beta'}\left(\widehat{\beta}\right)\right)^{-1}\left(f\left(\widehat{\beta}\right)-20\right)\stackrel{d}{\to}\chi_{1}^{2}.\)</span>$
This is a valid test with correct asymptotic size.</p>
<p>However, we can equivalently state the null hypothesis as
<span class="math notranslate nohighlight">\(\beta_{3}+40\beta_{4}=0\)</span> and we can construct a Wald statistic
accordingly. Asymptotically equivalent though, in general a linear
hypothesis is preferred to a nonlinear one, due to the approximation
error in the delta method under the null and more importantly the
invalidity of the Taylor expansion under the alternative. It also
highlights the problem of Wald test being <em>variant</em> to
re-parametrization.</p>
</section>
<section id="lagrangian-multiplier-test">
<h3>Lagrangian Multiplier Test<a class="headerlink" href="#lagrangian-multiplier-test" title="Permalink to this headline">#</a></h3>
<p>The key difference between the Wald test and LM test is that the former
is based on the unrestricted OLS estimator while the latter is based on
the restricted OLS estimator. Estimate the constrained OLS estimator
$<span class="math notranslate nohighlight">\(\min_{\beta}\left(y-X\beta\right)'\left(y-X\beta\right)\mbox{ s.t. }R\beta=r.\)</span><span class="math notranslate nohighlight">\(
We know that the restricted minimization problem can be converted into
an unrestricted problem
\)</span><span class="math notranslate nohighlight">\(L\left(\beta,\lambda\right)=\frac{1}{2n}\left(y-X\beta\right)'\left(y-X\beta\right)+\lambda'\left(R\beta-r\right),\label{eq:Lagran}\)</span><span class="math notranslate nohighlight">\(
where \)</span>L\left(\beta,\lambda\right)<span class="math notranslate nohighlight">\( is called the Lagrangian, and
\)</span>\lambda$ is the Lagrangian multiplier.</p>
<p>The LM test is also called the <em>score test</em>, because the derivation is
based on the score function of the restricted OLS estimator. Set the
first-order condition of
<a href="#eq:Lagran" data-reference-type="eqref" data-reference="eq:Lagran">[eq:Lagran]</a>
as zero: $<span class="math notranslate nohighlight">\(\begin{aligned}
\frac{\partial}{\partial\beta}L &amp; =-\frac{1}{n}X'\left(y-X\tilde{\beta}\right)+\tilde{\lambda}R=-\frac{1}{n}X'e+\frac{1}{n}X'X\left(\tilde{\beta}-\beta_{0}\right)+R'\tilde{\lambda}=0.\\
\frac{\partial}{\partial\lambda}L &amp; =R\tilde{\beta}-r=R\left(\tilde{\beta}-\beta_{0}\right)=0\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\tilde{\beta}<span class="math notranslate nohighlight">\( and \)</span>\tilde{\lambda}<span class="math notranslate nohighlight">\( denote the roots of these
equation, and \)</span>\beta_{0}<span class="math notranslate nohighlight">\( is the hypothesized true value. The two
equations can be written as a linear system
\)</span><span class="math notranslate nohighlight">\(\begin{pmatrix}\widehat{Q} &amp; R'\\
R &amp; 0
\end{pmatrix}\begin{pmatrix}\tilde{\beta}-\beta_{0}\\
\tilde{\lambda}
\end{pmatrix}=\begin{pmatrix}\frac{1}{n}X'e\\
0
\end{pmatrix},\)</span><span class="math notranslate nohighlight">\( where \)</span>\hat{Q}=X’X/n$.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}\widehat{Q}^{-1}-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1} &amp; \widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}\\
\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1} &amp; -(R'Q^{-1}R)^{-1}
\end{pmatrix}\begin{pmatrix}\widehat{Q} &amp; R'\\
R &amp; 0
\end{pmatrix}=I_{K+q}.\end{split}\]</div>
<p>Given the above fact, we can explicitly express $<span class="math notranslate nohighlight">\(\begin{aligned}
\begin{pmatrix}\tilde{\beta}-\beta_{0}\\
\tilde{\lambda}
\end{pmatrix}\begin{aligned}=\end{aligned}
 &amp; \begin{pmatrix}\widehat{Q}^{-1}-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1} &amp; \widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}\\
\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1} &amp; -(R'Q^{-1}R)^{-1}
\end{pmatrix}\begin{pmatrix}\frac{1}{n}X'e\\
0
\end{pmatrix}\\
= &amp; \begin{pmatrix}\widehat{Q}^{-1}\frac{1}{n}X'e-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{n}X'e\\
\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{n}X'e
\end{pmatrix}\end{aligned}\)</span><span class="math notranslate nohighlight">\( The \)</span>\tilde{\lambda}<span class="math notranslate nohighlight">\( component is
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\sqrt{n}\tilde{\lambda} &amp; =\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\\
 &amp; \stackrel{d}{\to}N\left(0,\left(RQ^{-1}R'\right)^{-1}RQ^{-1}\Omega Q^{-1}R'\left(RQ^{-1}R'\right)^{-1}\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
as \)</span>\widehat{Q}\stackrel{p}{\to}Q<span class="math notranslate nohighlight">\(. Denote
\)</span>\Sigma=\left(RQ^{-1}R’\right)^{-1}RQ^{-1}\Omega Q^{-1}R’\left(RQ^{-1}R’\right)^{-1}<span class="math notranslate nohighlight">\(,
we have
\)</span><span class="math notranslate nohighlight">\(n\tilde{\lambda}'\Sigma^{-1}\tilde{\lambda}\stackrel{d}{\to}\chi_{q}^{2}.\)</span><span class="math notranslate nohighlight">\(
Let
\)</span><span class="math notranslate nohighlight">\(\widehat{\Sigma}=\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\widehat{\Omega}\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}.\)</span><span class="math notranslate nohighlight">\(
If \)</span>\widehat{\Omega}\stackrel{p}{\to}\Omega<span class="math notranslate nohighlight">\(, we have \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\mathcal{LM} &amp; =n\tilde{\lambda}'\widehat{\Sigma}^{-1}\tilde{\lambda}=n\tilde{\lambda}'\Sigma^{-1}\tilde{\lambda}+n\tilde{\lambda}'\left(\widehat{\Sigma}^{-1}-\Sigma^{-1}\right)\tilde{\lambda}\\
 &amp; =n\tilde{\lambda}'\Sigma^{-1}\tilde{\lambda}+o_{p}\left(1\right)\stackrel{d}{\to}\chi_{q}^{2}.\end{aligned}\)</span>$
This is the general expression of the LM test.</p>
<p>In the special case of homoskedasticity,
<span class="math notranslate nohighlight">\(\Sigma=\sigma^{2}\left(RQ^{-1}R'\right)^{-1}RQ^{-1}QQ^{-1}R'\left(RQ^{-1}R'\right)^{-1}=\sigma^{2}\left(RQ^{-1}R'\right)^{-1}.\)</span>
Replace <span class="math notranslate nohighlight">\(\Sigma\)</span> with the estimated <span class="math notranslate nohighlight">\(\hat{\Sigma}\)</span>, we have
$<span class="math notranslate nohighlight">\(\begin{aligned}\frac{n\tilde{\lambda}'R\hat{Q}^{-1}R'\tilde{\lambda}}{\hat{\sigma}^{2}} &amp; =\frac{1}{n\hat{\sigma}^{2}}\left(y-X\tilde{\beta}\right)'X\hat{Q}^{-1}R'(R\hat{Q}^{-1}R')^{-1}R\hat{Q}^{-1}X'\left(y-X\tilde{\beta}\right)\stackrel{d}{\to}\chi_{q}^{2}.\end{aligned}\)</span>$</p>
<p>If we test the hypothesis that the optimal experience level is 20 years;
<span class="math notranslate nohighlight">\(\mbox{experience}^{*}=-\frac{\beta_{3}}{2\beta_{4}}=20.\)</span> We can replace
<span class="math notranslate nohighlight">\(\beta_{3}\)</span> by <span class="math notranslate nohighlight">\(-40\beta_{4}\)</span> so we only need to estimate 3 slope
coefficients in the OLS to construct the LM test. Moreover, the LM test
is invariant to re-parametrization.</p>
</section>
<section id="likelihood-ratio-test-for-regression">
<h3>Likelihood-Ratio Test for Regression<a class="headerlink" href="#likelihood-ratio-test-for-regression" title="Permalink to this headline">#</a></h3>
<p>In the previous section we have discussed the LRT. Here we put it into
the context regression with Gaussian error. Let <span class="math notranslate nohighlight">\(\gamma=\sigma_{e}^{2}\)</span>.
Under the classical assumptions of normal regression model,
$<span class="math notranslate nohighlight">\(L_{n}\left(\beta,\gamma\right)=-\frac{n}{2}\log\left(2\pi\right)-\frac{n}{2}\log\gamma-\frac{1}{2\gamma}\left(Y-X\beta\right)'\left(Y-X\beta\right).\)</span><span class="math notranslate nohighlight">\(
For the unrestricted estimator, we know
\)</span><span class="math notranslate nohighlight">\(\widehat{\gamma}=\gamma\left(\widehat{\beta}\right)=n^{-1}\left(Y-X\widehat{\beta}\right)'\left(Y-X\widehat{\beta}\right)\)</span><span class="math notranslate nohighlight">\(
and the sample log-likelihood function evaluated at the MLE is
\)</span><span class="math notranslate nohighlight">\(\widehat{L}_{n}=L_{n}\left(\widehat{\beta},\widehat{\gamma}\right)=-\frac{n}{2}\log\left(2\pi\right)-\frac{n}{2}\log\widehat{\gamma}-\frac{n}{2}\)</span><span class="math notranslate nohighlight">\(
and the restricted estimator
\)</span>\tilde{L}<em>{n}=L</em>{n}\left(\tilde{\beta},\tilde{\gamma}\right)=-\frac{n}{2}\log\left(2\pi\right)-\frac{n}{2}\log\tilde{\gamma}-\frac{n}{2}<span class="math notranslate nohighlight">\(.
The likelihood ratio is \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\mathcal{LR} &amp; =2\left(\widehat{L}_{n}-\tilde{L}_{n}\right)=n\log\left(\tilde{\gamma}/\widehat{\gamma}\right).\end{aligned}\)</span><span class="math notranslate nohighlight">\(
If the normal regression is correctly specified, we can immediately
conclude \)</span>\mathcal{LR}\stackrel{d}{\to}\chi_{q}^{2}.$</p>
<p>Now we drop the Gaussian error assumption while keep the conditional
homoskedasticity. In this case, the classical results is not applicable
because <span class="math notranslate nohighlight">\(L_{n}\left(\beta,\gamma\right)\)</span> is not a (genuine)
log-likelihood function; instead it is the <em>quasi log-likelihood
function</em>. Notice $<span class="math notranslate nohighlight">\(\begin{aligned}
\mathcal{LR} &amp; =n\log\left(1+\frac{\tilde{\gamma}-\widehat{\gamma}}{\widehat{\gamma}}\right)=n\left(\log1+\frac{\tilde{\gamma}-\widehat{\gamma}}{\widehat{\gamma}}+O\left(\frac{\left|\tilde{\gamma}-\widehat{\gamma}\right|^{2}}{\widehat{\gamma}^{2}}\right)\right)\nonumber \\
 &amp; =n\frac{\tilde{\gamma}-\widehat{\gamma}}{\widehat{\gamma}}+o_{p}\left(1\right)\label{eq:LRT1}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
by a Taylor expansion of
\)</span>\log\left(1+\frac{\tilde{\gamma}-\widehat{\gamma}}{\widehat{\gamma}}\right)<span class="math notranslate nohighlight">\(
around \)</span>\log1=0<span class="math notranslate nohighlight">\(. We focus on \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
n\left(\tilde{\gamma}-\widehat{\gamma}\right) &amp; =n\left(\gamma\left(\tilde{\beta}\right)-\gamma\left(\widehat{\beta}\right)\right)\nonumber \\
 &amp; =n\left(\frac{\partial\gamma\left(\widehat{\beta}\right)}{\partial\beta}\left(\tilde{\beta}-\widehat{\beta}\right)+\frac{1}{2}\left(\tilde{\beta}-\widehat{\beta}\right)'\frac{\partial^{2}\gamma\left(\widehat{\beta}\right)}{\partial\beta\partial\beta'}\left(\tilde{\beta}-\widehat{\beta}\right)+O\left(\left\Vert \tilde{\beta}-\widehat{\beta}\right\Vert _{2}^{3}\right)\right)\nonumber \\
 &amp; =\sqrt{n}\left(\tilde{\beta}-\widehat{\beta}\right)'\widehat{Q}\sqrt{n}\left(\tilde{\beta}-\widehat{\beta}\right)+o_{p}\left(1\right)\label{eq:LRT2}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where the last line follows by
\)</span>\frac{\partial\gamma\left(\widehat{\beta}\right)}{\partial\beta}=-\frac{2}{n}X’\left(Y-X\widehat{\beta}\right)=-\frac{2}{n}X’\widehat{e}=0<span class="math notranslate nohighlight">\(
and
\)</span>\frac{1}{2}\cdot\frac{\partial^{2}\gamma\left(\widehat{\beta}\right)}{\partial\beta\partial\beta’}=\frac{1}{2}\cdot\frac{2}{n}X’X=\widehat{Q}$.</p>
<p>From the derivation of LM test, we have
$<span class="math notranslate nohighlight">\(\begin{aligned}\sqrt{n}\left(\tilde{\beta}-\beta_{0}\right) &amp; =\left(\widehat{Q}^{-1}-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\right)\frac{1}{\sqrt{n}}X'e\\
 &amp; =\frac{1}{\sqrt{n}}\left(X'X\right)^{-1}X'e-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\\
 &amp; =\sqrt{n}\left(\widehat{\beta}-\beta_{0}\right)-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e.
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Rearrange the above equation to obtain
\)</span><span class="math notranslate nohighlight">\(\sqrt{n}\left(\tilde{\beta}-\widehat{\beta}\right)=-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\)</span><span class="math notranslate nohighlight">\(
and thus the quadratic form \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
 &amp;  &amp; \sqrt{n}\left(\tilde{\beta}-\widehat{\beta}\right)'\widehat{Q}\sqrt{n}\left(\tilde{\beta}-\widehat{\beta}\right)\nonumber \\
 &amp; = &amp; \frac{1}{\sqrt{n}}e'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\widehat{Q}\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\nonumber \\
 &amp; = &amp; \frac{1}{\sqrt{n}}e'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\nonumber \\
 &amp; = &amp; \frac{1}{\sqrt{n}}e'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e.\label{eq:LRT3}\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Collecting
(&lt;a href=&quot;#eq:LRT1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:LRT1&quot;&gt;[eq:LRT1]&lt;/a&gt;),
(&lt;a href=&quot;#eq:LRT2&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:LRT2&quot;&gt;[eq:LRT2]&lt;/a&gt;)
and
(&lt;a href=&quot;#eq:LRT3&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:LRT3&quot;&gt;[eq:LRT3]&lt;/a&gt;),
we have \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\mathcal{LR} &amp; =n\frac{\sigma_{e}^{2}}{\widehat{\gamma}}\cdot\frac{\tilde{\gamma}-\widehat{\gamma}}{\sigma_{e}^{2}}+o_{p}\left(1\right)\\
 &amp; =\frac{\sigma_{e}^{2}}{\widehat{\gamma}}\frac{1}{\sqrt{n}}\frac{e}{\sigma_{e}}'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'\frac{e}{\sigma_{e}}+o_{p}\left(1\right)\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Notice that under homoskedasticity, CLT gives \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'\frac{e}{\sigma_{e}} &amp; =R\widehat{Q}^{-1/2}\widehat{Q}^{-1/2}\frac{1}{\sqrt{n}}X'\frac{e}{\sigma_{e}}\\
 &amp; \stackrel{d}{\to}RQ^{-1/2}\times N\left(0,I_{K}\right)\sim N\left(0,RQ^{-1}R'\right),\end{aligned}\)</span><span class="math notranslate nohighlight">\(
and thus
\)</span><span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}}\frac{e}{\sigma_{e}}'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'\frac{e}{\sigma_{e}}\stackrel{d}{\to}\chi_{q}^{2}.\)</span><span class="math notranslate nohighlight">\(
Moreover, \)</span>\frac{\sigma_{e}^{2}}{\widehat{\gamma}}\stackrel{p}{\to}1<span class="math notranslate nohighlight">\(.
By Slutsky’s theorem, we conclude
\)</span><span class="math notranslate nohighlight">\(\mathcal{LR}\stackrel{d}{\to}\chi_{q}^{2}.\)</span>$ under homoskedasticity.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h2>
<p>Applied econometrics is a field obsessed of hypothesis testing, in the
hope to establish at least statistical association and ideally
causality. Hypothesis testing is a fundamentally important topic in
statistics. The states and the decisions in Table
<a href="#tab:Decisions-and-States" data-reference-type="ref" data-reference="tab:Decisions-and-States">[tab:Decisions-and-States]</a>
remind us the intrinsic connections with game theory in economics. I, a
game player, plays a sequential game against the “nature”.</p>
<p>Step0:<br />
The parameter space <span class="math notranslate nohighlight">\(\Theta\)</span> is partitioned into the null hypothesis
<span class="math notranslate nohighlight">\(\Theta_{0}\)</span> and the alternative hypothesis <span class="math notranslate nohighlight">\(\Theta_{1}\)</span> according to a
scientific theory.</p>
<p>Step1:<br />
Before I observe the data, I design a test function <span class="math notranslate nohighlight">\(\phi\)</span> according to
<span class="math notranslate nohighlight">\(\Theta_{0}\)</span> and <span class="math notranslate nohighlight">\(\Theta_{1}\)</span>. In game theory terminology, the
contingency plan <span class="math notranslate nohighlight">\(\phi\)</span> is my <em>strategy</em>.</p>
<p>Step2:<br />
Once I observe the fixed data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, I act according to the
instruction of <span class="math notranslate nohighlight">\(\phi\left(\mathbf{x}\right)\)</span> — either accept
<span class="math notranslate nohighlight">\(\Theta_{0}\)</span> or reject <span class="math notranslate nohighlight">\(\Theta_{0}\)</span>.</p>
<p>Step3:<br />
Nature reveals the true parameter <span class="math notranslate nohighlight">\(\theta^{*}\)</span> behind <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Then
I can evaluate the gain/loss of my decision
<span class="math notranslate nohighlight">\(\phi\left(\mathbf{x}\right)\)</span>.</p>
<p>When the loss function (negative payoff) is specified as
$<span class="math notranslate nohighlight">\(\mathscr{L}\left(\theta,\phi\left(\mathbf{x}\right)\right)=\phi\left(\mathbf{x}\right)\cdot1\left\{ \theta\in\Theta_{0}\right\} +\left(1-\phi\left(\mathbf{x}\right)\right)\cdot1\left\{ \theta\in\Theta_{1}\right\} ,\)</span><span class="math notranslate nohighlight">\(
the randomness of the data will incur the risk (expected loss)
\)</span><span class="math notranslate nohighlight">\(\mathscr{R}\left(\theta,\phi\right)=E\left[\mathscr{L}\left(\theta,\phi\left(\mathbf{x}\right)\right)\right]=\beta_{\phi}\left(\theta\right)\cdot1\left\{ \theta\in\Theta_{0}\right\} +\left(1-\beta_{\phi}\left(\theta\right)\right)\cdot1\left\{ \theta\in\Theta_{1}\right\} .\)</span>$
I am a rational person. I understand the structure of the game and I
want to do a good job in Step 1 in designing my strategy. I want to
minimize my risk.</p>
<p>If I am a frequentist, one and only one of
<span class="math notranslate nohighlight">\(1\left\{ \theta\in\Theta_{0}\right\}\)</span> and
<span class="math notranslate nohighlight">\(1\left\{ \theta\in\Theta_{1}\right\}\)</span> can happen. An unbiased test
makes sure
<span class="math notranslate nohighlight">\(\sup_{\theta\in\Theta_{0}}\beta_{\phi}\left(\theta\right)\leq\alpha\)</span>.
When many tests are unbiased, ideally I would like to pick the best one.
If it exists, in a class <span class="math notranslate nohighlight">\(\Psi_{\alpha}\)</span> of unbiased tests of size
<span class="math notranslate nohighlight">\(\alpha\)</span> the uniformly most power test <span class="math notranslate nohighlight">\(\phi^{*}\)</span> satisfies
<span class="math notranslate nohighlight">\(\mathscr{R}\left(\theta,\phi^{*}\right)\geq\sup_{\phi\in\Psi_{\alpha}}\mathscr{R}\left(\theta,\phi\right)\)</span>
for every <span class="math notranslate nohighlight">\(\theta\in\Theta_{1}\)</span>. For simple versus simple tests, LRT is
the uniformly most powerful test according to Neyman-Pearson Lemma.</p>
<p>If I am a Bayesian, I do not mind imposing probability (weight) on the
parameter space, which is my prior belief <span class="math notranslate nohighlight">\(\pi\left(\theta\right)\)</span>. My
Bayesian risk becomes $<span class="math notranslate nohighlight">\(\begin{aligned}
\mathscr{BR}\left(\pi,\phi\right) &amp; =E_{\pi\left(\theta\right)}\left[\mathscr{R}\left(\theta,\phi\right)\right]=\int\left[\beta_{\phi}\left(\theta\right)\cdot1\left\{ \theta\in\Theta_{0}\right\} +\left(1-\beta_{\phi}\left(\theta\right)\right)\cdot1\left\{ \theta\in\Theta_{1}\right\} \right]\pi\left(\theta\right)d\theta\\
 &amp; =\int_{\left\{ \theta\in\Theta_{0}\right\} }\beta_{\phi}\left(\theta\right)\pi\left(\theta\right)d\theta+\int_{\left\{ \theta\in\Theta_{1}\right\} }(1-\beta_{\phi}\left(\theta\right))\pi\left(\theta\right)d\theta.\end{aligned}\)</span><span class="math notranslate nohighlight">\(
This is the average (with respect to \)</span>\pi\left(\theta\right)$) risk over
the null and the alternative.</p>
<p><strong>Historical notes</strong>: Hypothesis testing started to take the modern
shape at the beginning of the 20th century. Karl Pearson (1957–1936)
laid the foundation of hypothesis testing and introduced the <span class="math notranslate nohighlight">\(\chi^{2}\)</span>
test, the <span class="math notranslate nohighlight">\(p\)</span>-value, among many other concepts that we keep using today.
Neyman-Pearson Lemma was named after Jerzy Neyman (1894–1981) and Egon
Pearson (1895–1980), Karl’s son.</p>
<p><strong>Further reading</strong>: &#64;young2005essentials is a concise but in-depth
reference for statistical inference.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 史震涛 Shi Zhentao<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>