
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Conditional Expectation &#8212; 计量经济学讲义</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">计量经济学讲义</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    前言
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lecture1-CN.html">
   1. 概率论
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture2-CN.html">
   2. 投影
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture3-CN.html">
   3. 最小二乘法：线性代数观点
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture4-CN.html">
   4. 最小二乘: 有限样本理论
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-CN.html">
   5. 基本渐近理论
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/zhentaoshi/metrics1-cn/master?urlpath=tree/docs/02-lecture.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/zhentaoshi/metrics1-cn"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/zhentaoshi/metrics1-cn/issues/new?title=Issue%20on%20page%20%2F02-lecture.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/02-lecture.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-projection">
   Linear Projection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omitted-variable-bias">
     Omitted Variable Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#causality">
   Causality
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structure-and-identification">
     Structure and Identification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#treatment-effect">
     Treatment Effect
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ate-and-cef">
     ATE and CEF
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Conditional Expectation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-projection">
   Linear Projection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omitted-variable-bias">
     Omitted Variable Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#causality">
   Causality
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structure-and-identification">
     Structure and Identification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#treatment-effect">
     Treatment Effect
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ate-and-cef">
     ATE and CEF
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="conditional-expectation">
<h1>Conditional Expectation<a class="headerlink" href="#conditional-expectation" title="Permalink to this headline">#</a></h1>
<p><strong>Notation</strong>: In this note, <span class="math notranslate nohighlight">\(y\)</span> is a scale random variable, and
<span class="math notranslate nohighlight">\(x=\left(x_{1},\ldots,x_{K}\right)'\)</span> is a <span class="math notranslate nohighlight">\(K\times1\)</span> random vector.
Throughout this course, a vector is a <em>column</em> vector, i.e. a one-column
matrix.</p>
<hr class="docutils" />
<p>Machine learning is a big basket that contains the regression models. We
motivate the conditional expectation model from the perspective of
prediction. We view a regression as <em>supervised learning</em>. Supervised
learning uses a function of <span class="math notranslate nohighlight">\(x\)</span>, say, <span class="math notranslate nohighlight">\(g\left(x\right)\)</span>, to predict <span class="math notranslate nohighlight">\(y\)</span>.
<span class="math notranslate nohighlight">\(x\)</span> cannot perfectly predict <span class="math notranslate nohighlight">\(y\)</span>; otherwise their relationship is
deterministic. The prediction error <span class="math notranslate nohighlight">\(y-g\left(x\right)\)</span> depends on the
choice of <span class="math notranslate nohighlight">\(g\)</span>. There are numerous possible choices of <span class="math notranslate nohighlight">\(g\)</span>. Which one is
the best? Notice that this question is not concerned about the
underlying data generating process (DGP) of the joint distribution of
<span class="math notranslate nohighlight">\(\left(y,x\right)\)</span>. We want to find a general rule to achieve accurate
prediction of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>, no matter how this pair of variables is
generated.</p>
<p>To answer this question, we need to decide a criterion to compare
different <span class="math notranslate nohighlight">\(g\)</span>. Such a criterion is called the <em>loss function</em>
<span class="math notranslate nohighlight">\(L\left(y,g\left(x\right)\right)\)</span>. A particularly convenient one is the
<em>quadratic loss</em>, defined as
$<span class="math notranslate nohighlight">\(L\left(y,g\left(x\right)\right)=\left(y-g\left(x\right)\right)^{2}.\)</span><span class="math notranslate nohighlight">\(
Since the data are random, \)</span>L\left(y,g\left(x\right)\right)<span class="math notranslate nohighlight">\( is also
random. &quot;Random&quot; means uncertainty: sometimes *this* happens, and
sometimes *that* happens. To get rid of the uncertainty, we average the
loss function with respect to the joint distribution of
\)</span>\left(y,x\right)<span class="math notranslate nohighlight">\( as
\)</span>R\left(y,g\left(x\right)\right)=E\left[L\left(y,g\left(x\right)\right)\right]<span class="math notranslate nohighlight">\(,
which is called *risk*. Risk is a deterministic quality. For the
quadratic loss function, the corresponding risk is
\)</span><span class="math notranslate nohighlight">\(R\left(y,g\left(x\right)\right)=E\left[\left(y-g\left(x\right)\right)^{2}\right],\)</span><span class="math notranslate nohighlight">\(
is called the *mean squared error* (MSE). MSE is the most widely used
risk measure, although there exist many alternative measures, for
example the *mean absolute error* (MAE)
\)</span>E\left[\left|y-g\left(x\right)\right|\right]<span class="math notranslate nohighlight">\(. The popularity of MSE
comes from its convenience for analysis in closed-form, which MAE does
not enjoy due to its nondifferentiability. This is similar to the choice
of utility functions in economics. There are only a few functional forms
for the utility, for example CRRA, CARA, and so on. They are popular
because they lead to close-form solutions that are easy to handle. Now
our quest is narrowed to: What is the optimal choice of \)</span>g$ if we
minimize the MSE?</p>
<p>[[prop:CEF]]{#prop:CEF label=”prop:CEF”} The conditional mean function
(CEF)
<span class="math notranslate nohighlight">\(m\left(x\right)=E\left[y|x\right]=\int yf\left(y|x\right)\mathrm{d}y\)</span>
minimizes MSE.</p>
<p>Before we prove the above proposition, we first discuss some properties
of the conditional mean function. Obviously
$<span class="math notranslate nohighlight">\(y=m\left(x\right)+\left(y-m\left(x\right)\right)=m\left(x\right)+\epsilon,\)</span><span class="math notranslate nohighlight">\(
where \)</span>\epsilon:=y-m\left(x\right)<span class="math notranslate nohighlight">\( is called the *regression error*.
This equation holds for \)</span>\left(y,x\right)<span class="math notranslate nohighlight">\( following any joint
distribution, as long as \)</span>E\left[y|x\right]<span class="math notranslate nohighlight">\( exists. The error term
\)</span>\epsilon$ satisfies these properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\left[\epsilon|x\right]=E\left[y-m\left(x\right)|x\right]=E\left[y|x\right]-m(x)=0\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(E\left[\epsilon\right]=E\left[E\left[\epsilon|x\right]\right]=E\left[0\right]=0\)</span>,</p></li>
<li><p>For any function <span class="math notranslate nohighlight">\(h\left(x\right)\)</span>, we have
$<span class="math notranslate nohighlight">\(E\left[h\left(x\right)\epsilon\right]=E\left[E\left[h\left(x\right)\epsilon|x\right]\right]=E\left[h(x)E\left[\epsilon|x\right]\right]=0.\label{eq:uncorr}\)</span>$</p></li>
</ul>
<p>The last property implies that <span class="math notranslate nohighlight">\(\epsilon\)</span> is uncorrelated with any
function of <span class="math notranslate nohighlight">\(x\)</span>. In particular, when <span class="math notranslate nohighlight">\(h\)</span> is the identity function
<span class="math notranslate nohighlight">\(h\left(x\right)=x\)</span>, we have
<span class="math notranslate nohighlight">\(E\left[x\epsilon\right]=\mathrm{cov}\left(x,\epsilon\right)=0\)</span>.</p>
<p>\bigskip{}
The optimality of the CEF can be confirmed by “guess-and-verify.” For an
arbitrary <span class="math notranslate nohighlight">\(g\left(x\right)\)</span>, the MSE can be decomposed into three terms
$<span class="math notranslate nohighlight">\(\begin{aligned}
 &amp;  &amp; E\left[\left(y-g\left(x\right)\right)^{2}\right]\\
 &amp; = &amp; E\left[\left(y-m(x)+m(x)-g(x)\right)^{2}\right]\\
 &amp; = &amp; E\left[\left(y-m\left(x\right)\right)^{2}\right]+2E\left[\left(y-m\left(x\right)\right)\left(m\left(x\right)-g\left(x\right)\right)\right]+E\left[\left(m\left(x\right)-g\left(x\right)\right)^{2}\right].\end{aligned}\)</span><span class="math notranslate nohighlight">\(
The first term is irrelevant to \)</span>g\left(x\right)<span class="math notranslate nohighlight">\(. The second term
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}2E\left[\left(y-m\left(x\right)\right)\left(m\left(x\right)-g\left(x\right)\right)\right] &amp; =2E\left[\epsilon\left(m\left(x\right)-g\left(x\right)\right)\right]=0\end{aligned}\)</span><span class="math notranslate nohighlight">\(
by invoking ([\[eq:uncorr\]](#eq:uncorr){reference-type=&quot;ref&quot;
reference=&quot;eq:uncorr&quot;}) with
\)</span>h\left(x\right)=m\left(x\right)-g\left(x\right)<span class="math notranslate nohighlight">\(. The second term is
again irrelevant of \)</span>g\left(x\right)<span class="math notranslate nohighlight">\(. The third term, obviously, is
minimized at \)</span>g\left(x\right)=m\left(x\right)$.</p>
<p>\bigskip{}
Our perspective so far deviates from many econometric textbooks that
assume that the dependent variable <span class="math notranslate nohighlight">\(y\)</span> is generated as
<span class="math notranslate nohighlight">\(g\left(x\right)+\epsilon\)</span> for some unknown function
<span class="math notranslate nohighlight">\(g\left(\cdot\right)\)</span> and error term <span class="math notranslate nohighlight">\(\epsilon\)</span> such that
<span class="math notranslate nohighlight">\(E\left[\epsilon|x\right]=0\)</span>. Instead, we take a predictive approach
regardless the DGP. What we observe are <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x\)</span> and we are solely
interested in seeking a function <span class="math notranslate nohighlight">\(g\left(x\right)\)</span> to predict <span class="math notranslate nohighlight">\(y\)</span> as
accurately as possible under the MSE criterion.</p>
<section id="linear-projection">
<h2>Linear Projection<a class="headerlink" href="#linear-projection" title="Permalink to this headline">#</a></h2>
<p>The CEF <span class="math notranslate nohighlight">\(m(x)\)</span> is the function that minimizes the MSE. However,
<span class="math notranslate nohighlight">\(m\left(x\right)=E\left[y|x\right]\)</span> is a complex function of <span class="math notranslate nohighlight">\(x\)</span>, for it
depends on the joint distribution of <span class="math notranslate nohighlight">\(\left(y,x\right)\)</span>, which is mostly
unknown in practice. Now let us make the prediction task even simpler.
How about we minimize the MSE within all linear functions in the form of
<span class="math notranslate nohighlight">\(h\left(x\right)=h\left(x;b\right)=x'b\)</span> for <span class="math notranslate nohighlight">\(b\in\mathbb{R}^{K}\)</span>? The
minimization problem is
$<span class="math notranslate nohighlight">\(\min_{b\in\mathbb{R}^{K}}E\left[\left(y-x'b\right)^{2}\right].\label{eq:linear_MSE}\)</span><span class="math notranslate nohighlight">\(
Take the first-order condition of the MSE
\)</span><span class="math notranslate nohighlight">\(\frac{\partial}{\partial b}E\left[\left(y-x'b\right)^{2}\right]=E\left[\frac{\partial}{\partial b}\left(y-x'b\right)^{2}\right]=-2E\left[x\left(y-x'b\right)\right],\)</span><span class="math notranslate nohighlight">\(
where the first equality holds if
\)</span>E\left[\left(y-x’b\right)^{2}\right]&lt;\infty<span class="math notranslate nohighlight">\( so that the expectation
and partial differentiation is interchangeable, and the second equality
hods by the chain rule and the linearity of expectation. Set the first
order condition to 0 and we solve
\)</span><span class="math notranslate nohighlight">\(\beta=\arg\min_{b\in\mathbb{R}^{K}}E\left[\left(y-x'b\right)^{2}\right]\)</span><span class="math notranslate nohighlight">\(
in the closed-form
\)</span><span class="math notranslate nohighlight">\(\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right]\)</span><span class="math notranslate nohighlight">\( if
\)</span>E\left[xx’\right]<span class="math notranslate nohighlight">\( is invertible. Notice here that \)</span>b<span class="math notranslate nohighlight">\( is an arbitrary
\)</span>K<span class="math notranslate nohighlight">\(-vector, while \)</span>\beta<span class="math notranslate nohighlight">\( is the optimizer. The function \)</span>x’\beta<span class="math notranslate nohighlight">\( is
called the *best linear projection* (BLP) of \)</span>y<span class="math notranslate nohighlight">\( on \)</span>x<span class="math notranslate nohighlight">\(, and the vector
\)</span>\beta$ is called the <em>linear projection coefficient</em>.</p>
<p>\bigskip{}
The linear function is not as restrictive as one might thought. It can
be used to produce some nonlinear (in random variables) effect if we
re-define <span class="math notranslate nohighlight">\(x\)</span>. For example, if
$<span class="math notranslate nohighlight">\(y=x_{1}\beta_{1}+x_{2}\beta_{2}+x_{1}^{2}\beta_{3}+e,\)</span><span class="math notranslate nohighlight">\( then
\)</span>\frac{\partial}{\partial x_{1}}m\left(x_{1},x_{2}\right)=\beta_{1}+2x_{1}\beta_{3}<span class="math notranslate nohighlight">\(,
which is nonlinear in \)</span>x_{1}<span class="math notranslate nohighlight">\(, while it is still linear in the parameter
\)</span>\beta=\left(\beta_{1},\beta_{2},\beta_{3}\right)<span class="math notranslate nohighlight">\( if we define a set of
new regressors as
\)</span>\left(\tilde{x}<em>{1},\tilde{x}</em>{2},\tilde{x}<em>{3}\right)=\left(x</em>{1},x_{2},x_{1}^{2}\right)$.</p>
<p>If <span class="math notranslate nohighlight">\(\left(y,x\right)\)</span> is jointly normal in the form $<span class="math notranslate nohighlight">\(\begin{pmatrix}y\\
x
\end{pmatrix}\sim\mathrm{N}\left(\begin{pmatrix}\mu_{y}\\
\mu_{x}
\end{pmatrix},\begin{pmatrix}\sigma_{y}^{2} &amp; \rho\sigma_{y}\sigma_{x}\\
\rho\sigma_{y}\sigma_{x} &amp; \sigma_{x}^{2}
\end{pmatrix}\right)\)</span><span class="math notranslate nohighlight">\( where \)</span>\rho<span class="math notranslate nohighlight">\( is the correlation coefficient, then
\)</span><span class="math notranslate nohighlight">\(E\left[y|x\right]=\mu_{y}+\rho\frac{\sigma_{y}}{\sigma_{x}}\left(x-\mu_{x}\right)=\left(\mu_{y}-\rho\frac{\sigma_{y}}{\sigma_{x}}\mu_{x}\right)+\rho\frac{\sigma_{y}}{\sigma_{x}}x,\)</span><span class="math notranslate nohighlight">\(
is a liner function of \)</span>x$. In this example, the CEF is linear.</p>
<p>Even though in general <span class="math notranslate nohighlight">\(m\left(x\right)\neq x'\beta\)</span>, the linear form
<span class="math notranslate nohighlight">\(x'\beta\)</span> is still useful in approximating <span class="math notranslate nohighlight">\(m\left(x\right)\)</span>. That is,
<span class="math notranslate nohighlight">\(\beta=\arg\min\limits _{b\in\mathbb{R}^{K}}E\left[\left(m(x)-x'b\right)^{2}\right]\)</span>.</p>
<p>The first-order condition gives
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial b}E\left[\left(m(x)-x'b\right)^{2}\right]=-2E[x(m(x)-x'b)]=0\)</span>.
Rearrange the terms and obtain <span class="math notranslate nohighlight">\(E[x\cdot m(x)]=E[xx']b\)</span>. When <span class="math notranslate nohighlight">\(E[xx']\)</span>
is invertible, we solve
$<span class="math notranslate nohighlight">\(\left(E\left[xx'\right]\right){}^{-1}E[x\cdot m(x)]=\left(E\left[xx'\right]\right){}^{-1}E[E[xy|x]]=\left(E\left[xx'\right]\right){}^{-1}E[xy]=\beta.\)</span><span class="math notranslate nohighlight">\(
Thus \)</span>\beta<span class="math notranslate nohighlight">\( is also the best linear approximation to \)</span>m\left(x\right)$
under MSE.</p>
<p>\bigskip{}
We may rewrite the linear regression model, or the <em>linear projection
model,</em> as $<span class="math notranslate nohighlight">\(\begin{array}[t]{c}
y=x'\beta+e\\
E[xe]=0,
\end{array}\)</span><span class="math notranslate nohighlight">\( where \)</span>e=y-x’\beta<span class="math notranslate nohighlight">\( is called the *linear projection
error*, to be distinguished from \)</span>\epsilon=y-m(x).$</p>
<p>Show (a) <span class="math notranslate nohighlight">\(E\left[xe\right]=0\)</span>. (b) If <span class="math notranslate nohighlight">\(x\)</span> contains a constant, then
<span class="math notranslate nohighlight">\(E\left[e\right]=0\)</span>.</p>
<section id="omitted-variable-bias">
<h3>Omitted Variable Bias<a class="headerlink" href="#omitted-variable-bias" title="Permalink to this headline">#</a></h3>
<p>We write the <em>long regression</em> as
$<span class="math notranslate nohighlight">\(y=x_{1}'\beta_{1}+x_{2}'\beta_{2}+\beta_{3}+e_{\beta},\)</span><span class="math notranslate nohighlight">\( and the
*short regression* as \)</span><span class="math notranslate nohighlight">\(y=x_{1}'\gamma_{1}+\gamma_{2}+e_{\gamma},\)</span><span class="math notranslate nohighlight">\(
where \)</span>e_{\beta}<span class="math notranslate nohighlight">\( and \)</span>e_{\gamma}<span class="math notranslate nohighlight">\( are the projection errors,
respectively. If \)</span>\beta_{1}<span class="math notranslate nohighlight">\( in the long regression is the parameter of
interest, omitting \)</span>x_{2}<span class="math notranslate nohighlight">\( as in the short regression will render
*omitted variable bias* (meaning \)</span>\gamma_{1}\neq\beta_{1}<span class="math notranslate nohighlight">\() unless
\)</span>x_{1}<span class="math notranslate nohighlight">\( and \)</span>x_{2}$ are uncorrelated.</p>
<p>We first demean all the variables in the two regressions, which is
equivalent as if we project out the effect of the constant. The long
regression becomes
$<span class="math notranslate nohighlight">\(\tilde{y}=\tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+\tilde{e}_{\beta},\)</span><span class="math notranslate nohighlight">\(
and the short regression becomes
\)</span><span class="math notranslate nohighlight">\(\tilde{y}=\tilde{x}_{1}'\gamma_{1}+\tilde{e}_{\gamma},\)</span>$ where <em>tilde</em>
denotes the demeaned variable.</p>
<p>Show <span class="math notranslate nohighlight">\(\tilde{e}_{\beta}=e_{\beta}\)</span> and <span class="math notranslate nohighlight">\(\tilde{e}_{\gamma}=e_{\gamma}\)</span>.</p>
<p>After demeaning, the cross-moment equals to the covariance. The short
regression coefficient
$<span class="math notranslate nohighlight">\(\begin{aligned}\gamma_{1} &amp; =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{y}\right]\\
 &amp; =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\left(\tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+\tilde{e}_{\beta}\right)\right]\\
 &amp; =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}\\
 &amp; =\beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2},
\end{aligned}\)</span><span class="math notranslate nohighlight">\( where the third line holds as
\)</span>E\left[\tilde{x}<em>{1}\tilde{e}</em>{\beta}\right]=0<span class="math notranslate nohighlight">\(. Therefore,
\)</span>\gamma_{1}=\beta_{1}<span class="math notranslate nohighlight">\( if and only if
\)</span>E\left[\tilde{x}<em>{1}\tilde{x}</em>{2}’\right]\beta_{2}=0<span class="math notranslate nohighlight">\(, which demands
either \)</span>E\left[\tilde{x}<em>{1}\tilde{x}</em>{2}’\right]=0<span class="math notranslate nohighlight">\( or \)</span>\beta_{2}=0$.</p>
<p>Show that
<span class="math notranslate nohighlight">\(E\left[\left(y-x_{1}'\beta_{1}-x_{2}'\beta_{2}-\beta_{3}\right)^{2}\right]\leq E\left[\left(y-x_{1}'\gamma_{1}-\gamma_{2}\right)^{2}\right]\)</span>.</p>
<p>Obviously we prefer to run the long regression to attain <span class="math notranslate nohighlight">\(\beta_{1}\)</span> if
possible, for it is a more general model than the short regression and
achieves no larger variance in the projection error. However, sometimes
<span class="math notranslate nohighlight">\(x_{2}\)</span> is unobservable so the long regression is unavailable. This
example of omitted variable bias is ubiquitous in applied econometrics.
Ideally we would like to directly observe some regressors but in reality
we do not have them at hand. We should be aware of the potential
consequence when the data are not as ideal as we have wished. When only
the short regression is available, in some cases we are able to sign the
bias, meaning that we can argue whether <span class="math notranslate nohighlight">\(\gamma_{1}\)</span> is bigger or
smaller than <span class="math notranslate nohighlight">\(\beta_{1}\)</span> based on our knowledge.</p>
</section>
</section>
<section id="causality">
<h2>Causality<a class="headerlink" href="#causality" title="Permalink to this headline">#</a></h2>
<section id="structure-and-identification">
<h3>Structure and Identification<a class="headerlink" href="#structure-and-identification" title="Permalink to this headline">#</a></h3>
<p>Unlike physical laws such as Einstein’s mass–energy equivalence
<span class="math notranslate nohighlight">\(E=mc^{2}\)</span> and Newton’s universal gravitation <span class="math notranslate nohighlight">\(F=Gm_{1}m_{2}/r^{2}\)</span>,
economic phenomena can rarely be summarized in such a minimalistic
style. When using experiments to verify physical laws, scientists often
manage to come up with smart design in which signal-to-noise ratio is so
high that small disturbances are kept at a negligible level. On the
contrary, economic laws do not fit a laboratory for experimentation.
What is worse, the subjects in economic studies — human beings — are
heterogeneous and with many features that are hard to control. People
from distinctive cultural and family backgrounds respond to the same
issue differently and researchers can do little to homogenize them. The
signal-to-noise ratios in economic laws are often significantly lower
than those of physical laws, mainly due to the lack of laboratory
setting and the heterogeneous nature of the subjects.</p>
<p>Educational return and the demand-supply system are two classical topics
in econometrics. A person’s incomes is determined by too many random
factors in the academic and career path that is impossible to
exhaustively observe and control. The observable prices and quantities
are outcomes of equilibrium so the demand and supply affect each other.</p>
<p>Generations of thinkers have been debating the definitions of causality.
In economics, an accepted definition is <em>structural causality</em>.
Structural causality is a thought experiment. It assumes that there is a
DGP that produces the observational data. If we can use data to recover
the DGP or some features of the DGP, then we have learned causality or
some implications of causality.</p>
<p>A key issue to resolve before looking at the realized sample is
<em>identification</em>. We say a model or DGP is <em>identified</em> if the each
possible parameter of the model under consideration generates
distinctive features of the observable data. A model is
<em>under-identified</em> if more than one parameter in the model can generate
exact the same features of the observable data. In other words, a model
is under-identified if from the observable data we cannot trace back to
a unique parameter in the model. A correctly specified model is the
prerequisite for any discussion of identification. In reality, all
models are wrong. Thus when talking about identification, we are
indulged in an imaginary world. If in such a thought experiment we still
cannot unique distinguish the true parameter of the data generating
process, then identification fails. We cannot determine what is the true
model no matter how large the sample is.</p>
</section>
<section id="treatment-effect">
<h3>Treatment Effect<a class="headerlink" href="#treatment-effect" title="Permalink to this headline">#</a></h3>
<p>We narrow down to the framework of the relationship between <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x\)</span>.
One question of particular interest is <em>treatment effect</em>. The treatment
effect is how much <span class="math notranslate nohighlight">\(y\)</span> will change if we change a variable of interest,
say <span class="math notranslate nohighlight">\(d\)</span>, by one unit while keeping all other variables (including the
unobservable variables) the same. The Latin phrase <em>ceteris paribus</em>
means “keep all other things constant.”</p>
<p>During the 2020 covid-19 pandemic, Hong Kong’s unemployment rate rose to
a high-level and consumption collapsed. In order to boost the economy,
some Hong Kong residents were qualified in receiving 10,000 HKD cash
allowance from the government. We are interested in learning how much
does the 10,000 HKD allowance increase people’s consumption. For an
individual, we imagine two parallel worlds: one with the cash allowance
and one without. The difference of the consumption in the world with the
allowance, denoted <span class="math notranslate nohighlight">\(Y\left(1\right)\)</span>, and that in the world without the
allowance, denoted <span class="math notranslate nohighlight">\(Y\left(0\right)\)</span>, is the treatment effect of that
particular person. This thought experiment is called the <em>potential
outcome framework</em>.</p>
<p>However, in reality one and only one scenario happens, which echoes the
saying of ancient Greek philosopher Heraclitus (553 BC–475 BC) “You
cannot step into the same river twice.” The individual treatment effect
is not operational (<em>operational</em> means it can be computed from data at
the population level), because one and only one outcome is realized.
With many people available, we can define <em>average treatment effect</em>
(ATE) as
$<span class="math notranslate nohighlight">\(ATE=E\left[Y\left(1\right)-Y\left(0\right)\right]=E\left[Y\left(1\right)\right]-E\left[Y\left(0\right)\right].\)</span><span class="math notranslate nohighlight">\(
Notice that \)</span>E\left[Y\left(1\right)\right]<span class="math notranslate nohighlight">\( and
\)</span>E\left[Y\left(0\right)\right]<span class="math notranslate nohighlight">\( are still not operational before we
observe a companion variable
\)</span><span class="math notranslate nohighlight">\(D=1\left\{ \mbox{treatment received}\right\} .\)</span><span class="math notranslate nohighlight">\( Once each
individual's treatment status is observable,
\)</span>E\left[Y\left(1\right)|D=1\right]<span class="math notranslate nohighlight">\( and
\)</span>E\left[Y\left(0\right)|D=0\right]$ are operational from the data.</p>
<p>If the two potential outcomes
<span class="math notranslate nohighlight">\(\left(Y\left(1\right),Y\left(0\right)\right)\)</span> are independent of the
assignment <span class="math notranslate nohighlight">\(D\)</span>, then
<span class="math notranslate nohighlight">\(E\left[Y\left(1\right)\right]=E\left[Y\left(1\right)|D=1\right]\)</span> and
<span class="math notranslate nohighlight">\(E\left[Y\left(0\right)\right]=E\left[Y\left(0\right)|D=0\right]\)</span> so
that ATE can be estimated from the data in an operational way as
$<span class="math notranslate nohighlight">\(ATE=E\left[Y\left(1\right)|D=1\right]-E\left[Y\left(0\right)|D=0\right].\)</span><span class="math notranslate nohighlight">\(
Therefore, to evaluate ATE ideally we would like use a lottery to
randomly decide that some people receive the treatment (*treatment
group*, with \)</span>D=1<span class="math notranslate nohighlight">\() and the others do not (*control group*, with \)</span>D=0$).</p>
<p>When we have other control variables, we can also define a finer
treatment effect conditional on <span class="math notranslate nohighlight">\(x\)</span>:
$<span class="math notranslate nohighlight">\(ATE\left(x\right)=E\left[Y\left(1\right)|x\right]-E\left[Y\left(0\right)|x\right].\)</span><span class="math notranslate nohighlight">\(
ATE is the average effect in the population of individuals when we
hypothetical give them the treatment, keeping all other factors \)</span>x<span class="math notranslate nohighlight">\(
constant. If conditioning on \)</span>x<span class="math notranslate nohighlight">\(, the treatment \)</span>D<span class="math notranslate nohighlight">\( is independent of
\)</span>\left(Y\left(1\right),Y\left(0\right)\right)<span class="math notranslate nohighlight">\(, then ATE becomes
operational:
\)</span><span class="math notranslate nohighlight">\(ATE\left(x\right)=E\left[Y\left(1\right)|D=1,x\right]-E\left[Y\left(0\right)|D=0,x\right]\)</span><span class="math notranslate nohighlight">\(
The important condition
\)</span>\left(\left(Y\left(1\right),Y\left(0\right)\right)\perp D\right)|x$ is
called the <em>conditional independence assumption</em> (CIA).</p>
<p>CIA is more plausible than full independence. Consider the example
<span class="math notranslate nohighlight">\(Y\left(1\right)=x+u\left(1\right)\)</span>, <span class="math notranslate nohighlight">\(Y\left(0\right)=x+u\left(0\right)\)</span>
and <span class="math notranslate nohighlight">\(D=1\left\{ x+u_{d}\geq0\right\}\)</span>. If
<span class="math notranslate nohighlight">\(\left(\left(u\left(0\right),u\left(1\right)\right)\perp u_{d}\right)|x\)</span>,
then CIA is satisfied. Nevertheless
<span class="math notranslate nohighlight">\(\left(Y\left(1\right),Y\left(0\right)\right)\)</span> and <span class="math notranslate nohighlight">\(D\)</span> are statistically
dependent, since <span class="math notranslate nohighlight">\(x\)</span> is involved in all random variables.</p>
</section>
<section id="ate-and-cef">
<h3>ATE and CEF<a class="headerlink" href="#ate-and-cef" title="Permalink to this headline">#</a></h3>
<p>In the previous section the treatment <span class="math notranslate nohighlight">\(D\)</span> is binary. Now we consider a
continuous treatment <span class="math notranslate nohighlight">\(D\)</span>. Suppose the DGP, or the structural model, is
<span class="math notranslate nohighlight">\(Y=h\left(D,x,u\right)\)</span> where <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(x\)</span> are observable and <span class="math notranslate nohighlight">\(u\)</span> is
unobservable. It is natural to define ATE with the continuous treatment
(Hansen’s book Chapter 2.30 calls it <em>average causal effect</em>) as
$<span class="math notranslate nohighlight">\(ATE\left(d,x\right)=E\left[\lim_{\Delta\to0}\frac{h\left(d+\Delta,x,u\right)-h\left(d,x,u\right)}{\Delta}\right]=E\left[\frac{\partial}{\partial d}h\left(d,x,u\right)\right],\)</span><span class="math notranslate nohighlight">\(
where the continuous differentiability of \)</span>h\left(d,x,u\right)<span class="math notranslate nohighlight">\( at \)</span>d<span class="math notranslate nohighlight">\(
is implicitly assumed. Unlike the binary treatment case, here \)</span>d<span class="math notranslate nohighlight">\(
explicitly shows up in \)</span>ATE\left(d,x\right)<span class="math notranslate nohighlight">\( because the effect can vary
at different values of \)</span>d<span class="math notranslate nohighlight">\(. ATE here is the average effect in the
population of individuals if we hypothetical move \)</span>D<span class="math notranslate nohighlight">\( a tiny bit around
\)</span>d<span class="math notranslate nohighlight">\(, keeping all other factors \)</span>x$ constant.</p>
<p>In the previous sections, we focused on the CEF <span class="math notranslate nohighlight">\(m\left(d,x\right)\)</span>,
where <span class="math notranslate nohighlight">\(d\)</span> is added to <span class="math notranslate nohighlight">\(x\)</span> as an additional variable of interest. We did
not intend to model the underlying economic mechanism
<span class="math notranslate nohighlight">\(h\left(D,x,u\right)\)</span>, which may be very complex. Can we learn the
<span class="math notranslate nohighlight">\(ATE\left(d,x\right)\)</span> which bears the structural causal interpretation,
from the mechanical <span class="math notranslate nohighlight">\(m\left(d,x\right)\)</span> which merely cares about best
prediction? The answer is positive under CIA: <span class="math notranslate nohighlight">\(\left(u\perp D\right)|x\)</span>.
$<span class="math notranslate nohighlight">\(\begin{aligned}
\frac{\partial}{\partial d}m\left(d,x\right) &amp; =\frac{\partial}{\partial d}E\left[y|d,x\right]=\frac{\partial}{\partial d}E\left[h\left(d,x,u\right)|d,x\right]=\frac{\partial}{\partial d}\int h\left(d,x,u\right)f\left(u|d,x\right)du\\
 &amp; =\int\frac{\partial}{\partial d}\left[h\left(d,x,u\right)f\left(u|d,x\right)\right]du\\
 &amp; =\int\left[\frac{\partial}{\partial d}h\left(d,x,u\right)\right]f\left(u|d,x\right)du+\int h\left(d,x,u\right)\left[\frac{\partial}{\partial d}f\left(u|d,x\right)\right]du,\end{aligned}\)</span><span class="math notranslate nohighlight">\(
where the second line implicitly assumes interchangeability between the
integral and the partial derivative. Under CIA,
\)</span>\frac{\partial}{\partial d}f\left(u|d,x\right)=0<span class="math notranslate nohighlight">\( and the second term
drops out. Thus
\)</span><span class="math notranslate nohighlight">\(\frac{\partial}{\partial d}m\left(d,x\right)=\int\left[\frac{\partial}{\partial d}h\left(d,x,u\right)\right]f\left(u|d,x\right)du=E\left[\frac{\partial}{\partial d}h\left(d,x,u\right)\right]=ATE\left(d,x\right).\)</span><span class="math notranslate nohighlight">\(
This is an important result. It says that if CIA holds, we can learn the
causal effect of \)</span>d<span class="math notranslate nohighlight">\( on \)</span>y<span class="math notranslate nohighlight">\( by the partial derivative of CEF conditional
on \)</span>x<span class="math notranslate nohighlight">\(. In particular, if we further assume a linear CEF
\)</span>m\left(d,x\right)=\beta_{d}d+\beta_{x}’x<span class="math notranslate nohighlight">\(, then the causal effect is
the coefficient \)</span>\beta_{d}$.</p>
<p>CIA is the key condition that links the CEF and the causal effect. CIA
is not an innocuous assumption. In applications, our causal results are
credible only when we can convincing defend CIA.</p>
<p>Let factories’ output be a Cobb-Douglas function
<span class="math notranslate nohighlight">\(Y=AK^{\alpha}L^{\beta}\)</span>, where the capital level <span class="math notranslate nohighlight">\(K\)</span> and labor <span class="math notranslate nohighlight">\(L\)</span> as
well as the output <span class="math notranslate nohighlight">\(Y\)</span> is observable, while the “technology” <span class="math notranslate nohighlight">\(A\)</span> is
unobservable. Take logarithm on both sides of the equation:
$<span class="math notranslate nohighlight">\(y=u+\alpha k+\beta l\label{eq:causal}\)</span><span class="math notranslate nohighlight">\( where \)</span>y=\log Y<span class="math notranslate nohighlight">\(, \)</span>u=\log A<span class="math notranslate nohighlight">\(,
\)</span>k=\log K<span class="math notranslate nohighlight">\( and \)</span>l=\log L<span class="math notranslate nohighlight">\(. Suppose \)</span>\begin{pmatrix}u\
k\
l
\end{pmatrix}\sim N\left(\begin{pmatrix}1\
1\
1
\end{pmatrix},\begin{pmatrix}1 &amp; 0.5 &amp; 0\
0.5 &amp; 1 &amp; 0\
0 &amp; 0 &amp; 1
\end{pmatrix}\right)<span class="math notranslate nohighlight">\( and \)</span>\alpha=\beta=1/2<span class="math notranslate nohighlight">\( make the true DGP. Here \)</span>u<span class="math notranslate nohighlight">\(
and \)</span>k$ are correlated, because factories of larger scale can afford
robots to facilitate automation.</p>
<ol class="simple">
<li><p>What is the partial derivative of CEF when we use <span class="math notranslate nohighlight">\(k\)</span> as a treatment
variable for a fixed labor level <span class="math notranslate nohighlight">\(l\)</span>? (Hint: the CEF is a linear
function thanks to the joint normality.)</p></li>
<li><p>Does it coincide with <span class="math notranslate nohighlight">\(\alpha=1/2\)</span>, the coefficient in the causal
model (<a class="reference external" href="#eq:causal">[eq:causal]</a>{reference-type=”ref”
reference=”eq:causal”})? (Hint: No, because CIA is violated.)</p></li>
</ol>
<p>Sometimes applied researchers assume by brute force that
<span class="math notranslate nohighlight">\(y=m\left(d,x\right)+u\)</span> is the DGP and <span class="math notranslate nohighlight">\(E\left[u|d,x\right]=0\)</span>, where
<span class="math notranslate nohighlight">\(d\)</span> is the variable of interest and <span class="math notranslate nohighlight">\(x\)</span> is the vector of other control
variables. Under these assumptions,
$<span class="math notranslate nohighlight">\(ATE\left(d,x\right)=E\left[\frac{\partial}{\partial d}\left(m\left(d,x\right)+u\right)|d,x\right]=\frac{\partial m\left(d,x\right)}{\partial d}+\frac{\partial}{\partial d}E\left[u|d,x\right]=\frac{\partial m\left(d,x\right)}{\partial d},\)</span><span class="math notranslate nohighlight">\(
where the second equality holds if
\)</span>\frac{\partial}{\partial d}E\left[u|d,x\right]=E\left[\frac{\partial}{\partial d}u|d,x\right]<span class="math notranslate nohighlight">\(.
At a first glance, it seems that the mean independence assumption
\)</span>E\left[u|d,x\right]=0<span class="math notranslate nohighlight">\(, which is weaker than CIA, implies the
equivalence between \)</span>ATE\left(d,x\right)<span class="math notranslate nohighlight">\( and
\)</span>\partial m\left(d,x\right)/\partial d<span class="math notranslate nohighlight">\( here. However, such slight
weakening is achieved by a very strong assumption that the DGP
\)</span>h\left(d,x,u\right)<span class="math notranslate nohighlight">\( follows the additive separable form
\)</span>m\left(d,x\right)+u<span class="math notranslate nohighlight">\(. Without economic theory to defend the choice of
the assumed DGP \)</span>y=m\left(d,x\right)+u$, this is at best the
<em>reduced-form</em> approach.</p>
<p>The <em>structural approach</em> here models the economic mechanism, guided by
economic theory. The <em>reduced-form approach</em> is convenient and can
document stylized facts when suitable economic theory is not immediately
available. There are constant debates about the pros and cons of the two
approaches; see <em>Journal of Economic Perspectives</em> Vol. 24, No. 2 Spring
2010. In macroeconomics, the so-called Phillips curve, attributed to
A.W. Phillips about the negative correlation between inflation and
unemployment, is a stylized fact learned from the reduced-form approach.
The Lucas critique [&#64;lucas1976econometric] exposed its lack of
microfoundation and advocated modeling deep parameters that are
invariant to policy changes. The latter is a structural approach.
Ironically, more than 40 years has passed since the Lucas critique,
equations with little microfoundation still dominate the analytical
apparatus of central bankers.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h2>
<p>In this lecture, we cover the conditional mean function and causality.
When we are faced with a pair of random variable <span class="math notranslate nohighlight">\(\left(y,x\right)\)</span>
drawn from some joint distribution, the CEF is the best predictor. When
we go further into the structural causality about some treatment <span class="math notranslate nohighlight">\(d\)</span> to
the dependent variable <span class="math notranslate nohighlight">\(y\)</span>, under CIA we can find equivalence between
ATE and the partial derivative of CEF. All analyses are conducted in
population. We have not touched the sample yet.</p>
<p><strong>Historical notes</strong>: Regressions and conditional expectations are
concepts from statistics and they are imported to econometrics in early
time. Researchers at the Cowles Commission (now Cowles Foundation for
Research in Economics) — Jacob Marschak (1898–1977), Tjalling
Koopmans (1910–1985, Nobel Prize 1975), Trygve Haavelmo (1911–1999,
Nobel Prize 1989) and their colleagues — were trailblazers of the
econometric structural approach.</p>
<p>The potential outcome framework is not peculiar to economics. It is
widely used in other fields such as biostatistics and medical studies.
It was initiated by Jerzy Neyman (1894–1981) and extended by Donald
B. Rubin (1943– ), Professor of Statistics at Tsinghua University.</p>
<p><strong>Further reading</strong>: &#64;lewbel2019identification offers a comprehensive
summary of identification in econometrics. Accounting is an applied
field with many claimed causal inference drawn from simple regressions;
it is encouraging to hear &#64;gow2016causal to reflect causality in their
practices.</p>
<p>\bigskip
<code class="docutils literal notranslate"> <span class="pre">Zhentao</span> <span class="pre">Shi.</span> <span class="pre">Sep</span> <span class="pre">17,</span> <span class="pre">2020</span></code></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 史震涛 Shi Zhentao<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>