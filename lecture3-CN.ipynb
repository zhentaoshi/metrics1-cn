{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法：线性代数观点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} 《九章算术》第八章：方程\n",
    "今有卖牛二、羊五, 以买十三豕, 有馀钱一千. 卖牛三、豕三, 以买九羊, 钱适足. 卖羊六、豕八, 以买五牛, 钱不足六百. 问牛、羊、豕价各几何?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\\begin{array}{r} \n",
    "2 & 5 & -13\\\\\n",
    "3 & -9 & 3\\\\\n",
    "-5 & 6 & 8\n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c}\n",
    "牛\\\\\n",
    "羊\\\\\n",
    "猪\n",
    "\\end{array} \\right]\n",
    "=\n",
    "\\left[\\begin{array}{r}\n",
    "1000\\\\\n",
    "0\\\\\n",
    "-600\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "A = matrix(c(2, 3, -5, 5, -9, 6, -13, 3, 8), nrow = 3)\n",
    "b = c(1000, 0, -600)\n",
    "solve(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "最小二乘法 (OLS) 是计量经济学中最基本的估计模型, 它简单透明, 易于理解. 了解最小二乘法, 有助于我们研究更复杂的线性估计方法. 此外, 许多非线性估计量在真实值附近与线性估计量的行为是类似的. 在本讲义中, 我们将从线性代数的运算讲起, 学习一系列的知识. \n",
    "\n",
    "\n",
    "套用Leopold Kronecker的名言 \"上帝创造了整数, 其他都是人的作品\", 我想说 \"高斯创造了最小二乘法, 其他都是应用研究者的作品\". 在科学界, 最小二乘法的受欢迎程度远远超出了人们的想象. 但要注意的是, 最小二乘法是一种纯统计学的方法, 或者说是一种监督式机器学习的方法, 因此它揭示的是相关关系, 而非因果关系. 相反, 经济理论假设因果关系的存在, 然后我们收集数据来检验理论或量化效果. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学标记: $y_{i}$ 是标量. $x_{i}=\\left(x_{i1},\\ldots,x_{iK}\\right)'$ 是 $K\\times1$ 的向量. $Y=\\left(y_{1},\\ldots,y_{n}\\right)'$ 是 $ n\\times1$ 的向量. \n",
    "\n",
    "$$ \n",
    "X=\\left[\\begin{array}{c}\n",
    "x_{1}'\\\\\n",
    "x_{2}'\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}'\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\cdots & x_{1K}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2K}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{22} & \\cdots & x_{nK}\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "是 $n\\times K$ 的矩阵. $I_{n}$ 是 $n\\times n$ 的单位矩阵. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘法\n",
    "\n",
    "给定 $n$ 个观测值 $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$, 我们想利用 $X$ 的某个线性组合 $Xb$ 尽量逼近 $Y$. 说到逼近，我们需要定义两个向量 $Y$ 和 $Xb$ 之间的距离. 欧氏距离(Euclidean distance) $\\Vert Y - Xb \\Vert = \\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}}$ 是常用的距离定义. \n",
    "定义一个目标函数\n",
    "\n",
    "$$\n",
    "Q\\left(b\\right)=\\frac{1}{2n}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}=\\frac{1}{2n}\\left(Y-Xb\\right)'\\left(Y-Xb\\right)=\\frac{1}{2n}\\left\\Vert Y-Xb\\right\\Vert ^{2}.\n",
    "$$\n",
    "\n",
    "残差平方和 $\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}$ 是 $\\Vert Y - Xb \\Vert$ 的平方。因为平方 $(\\cdot)^2$ 在 $\\mathbb{R}^+$上是严格单调递增函数，所以对 $b$ 最小化 $Q(b)$ 等价于最小化欧氏距离. 系数 $\\frac{1}{2n}$ 与 $b$ 无关, 不影响该最优化问题的解.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "最小化 $Q\\left(b\\right)$ 的一阶必要条件是\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b}Q\\left(b\\right)=\\left[\\begin{array}{c}\n",
    "\\partial Q\\left(b\\right)/\\partial b_{1}\\\\\n",
    "\\partial Q\\left(b\\right)/\\partial b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\partial Q\\left(b\\right)/\\partial b_{K}\n",
    "\\end{array}\\right]=-\\frac{1}{n}X'\\left(Y-Xb\\right)=0.\n",
    "$$\n",
    "\n",
    "对上式移项可得\n",
    "\n",
    "$$\n",
    "(X'X)b = X'Y.\n",
    "$$\n",
    "\n",
    "我们记该最优解 \n",
    "\n",
    "$$\n",
    "\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y.\n",
    "$$ \n",
    "\n",
    "另外, 二阶条件\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2}}{\\partial b\\partial b'}Q\\left(b\\right)=\\left[\\begin{array}{cccc}\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}^{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{1}}Q\\left(b\\right)\\\\\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}^{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{2}}Q\\left(b\\right)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{K}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{K}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}^{2}}Q\\left(b\\right)\n",
    "\\end{array}\\right]=\\frac{1}{n}X'X\n",
    "$$\n",
    "\n",
    "表明 $Q\\left(b\\right)$ 是关于 $b$ 的凸函数, 因为 $X'X/n$ 是半正定矩阵. (如果 $X'X/n$ 是正定矩阵, 那么 $Q\\left(b\\right)$ 是关于 $b$ 的严格凸函数. ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:remark}\n",
    ":label: remark3\n",
    "在最小二乘法的推导中, 我们假定 $X$ 中的 $K$ 列是*线性独立的*, 也就是说不存在 $K\\times1$ 的向量\n",
    "$b\\ (b\\neq0_{K})$ 使得 $Xb=0_{n}$. 同时, 这也意味着 $n\\geq K$ , 并且 $X'X/n$ 是可逆矩阵. \n",
    "\n",
    "然而线性独立的假设并不永远正确, 如果某些回归因子满足*完全共线性 (perfectly collinear)* , 就违反了线性独立的假设. \n",
    "\n",
    "例如, 使用虚拟变量来表示分类变量并将所有这些类别放入回归模型中时, 通常计量经济学软件会自动检测并指出完全共线性问题. 然而, 难以察觉的是*不完全共线性 (nearly collinear)*, 即 $X'X/n$ 的最小特征值接近于0, 而不等于0. 我们将在渐进理论一章中讨论不完全共线性的后果. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:property}\n",
    ":label: property1\n",
    "\n",
    "下面列举一些与最小二乘法估计量有关的定义与性质. \n",
    "\n",
    "-  拟合值 (Fitted value): $\\widehat{Y}=X\\widehat{\\beta}$.\n",
    "\n",
    "-  投影矩阵 (Projection matrix): $P_{X}=X\\left(X'X\\right)^{-1}X$; 残差生成矩阵 (Residual maker\n",
    "  matrix) : $M_{X}=I_{n}-P_{X}$.\n",
    "\n",
    "-  $P_{X}X=X$; $X'P_{X}=X'$.\n",
    "\n",
    "-  $M_{X}X=0_{n\\times K}$; $X'M_{X}=0_{K\\times n}$.\n",
    "\n",
    "-  $P_{X}M_{X}=M_{X}P_{X}=0_{n\\times n}$.\n",
    "\n",
    "-  $P_{X}$ 与 $M_{X}$ 都是*幂等矩阵*. \n",
    "  *  如果 $AA=A$, 那么 $A$ 被称作*幂等矩阵* (*idempotent* matrix). 幂等矩阵的特征值只能是1或者0. \n",
    "\n",
    "-  $\\mathrm{rank}\\left(P_{X}\\right)=K$, \n",
    "  $\\mathrm{rank}\\left(M_{X}\\right)=n-K$.\n",
    "\n",
    "- 残差: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{e}&=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y\\\\\n",
    "&=M_{X}\\left(X\\beta+e\\right)=M_{X}e.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "  注意 $\\widehat{e}$ 和 $e$ 是不同的. \n",
    "\n",
    "-  $X'\\widehat{e}=X'M_{X}e=0_{K}$.\n",
    "\n",
    "-  若 $x_{i}$中有一个为常数, 则 $\\sum_{i=1}^{n}\\widehat{e}_{i}=0$. \n",
    "\n",
    "  (因为 $X'\\widehat{e}=\\left[\\begin{array}{cccc}\n",
    "  1 & 1 & \\cdots & 1\\\\\n",
    "  \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\\\\\n",
    "  \\cdots & \\cdots & \\ddots & \\vdots\\\\\n",
    "  \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\n",
    "  \\end{array}\\right]\\left[\\begin{array}{c}\n",
    "  \\widehat{e}_{1}\\\\\n",
    "  \\widehat{e}_{2}\\\\\n",
    "  \\vdots\\\\\n",
    "  \\widehat{e}_{n}\n",
    "  \\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  \\vdots\\\\\n",
    "  0\n",
    "  \\end{array}\\right]$ , 第一行运算说明了\n",
    "  $\\sum_{i=1}^{n}\\widehat{e}_{i}=0$. \"$\\heartsuit$\" 代表与我们计算无关的值. )\n",
    "  \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We will produce a graph here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以从几何学的角度来理解最小二乘法. \n",
    "\n",
    "注意到, \n",
    "$\\mathcal{X}=\\left\\{ Xb:b\\in\\mathbb{R}^{K}\\right\\}$ 是一个由 $X=\\left[X_{\\cdot1},\\ldots,X_{\\cdot K}\\right]$ 中的 $K$ 列生成的线性空间; 如果这些列是线性独立的, 那么 $X$ 就是 $K$ 维的. 最小二乘法估计量 $\\widehat \\beta$ 使得 $\\left\\Vert Y-Xb\\right\\Vert^2$ 最小化, 也就是说使得 $\\left\\Vert Y-Xb\\right\\Vert$ 最小化 (对于 $a\\geq 0$, $a^2$ 是一个单调变换, 不影响最小化的结果) . 换言之, $X\\widehat{\\beta}$ 是 $\\mathcal{X}$ 中的一个点——与 $Y$ 最接近的一个点. \n",
    "\n",
    "\n",
    "等式 $Y=X\\widehat{\\beta}+\\widehat{e}$ 将 $Y$ 分解为两个垂直向量, $X\\widehat{\\beta}$ 与 $\\widehat{e}$, 因为$\\left\\langle X\\widehat{\\beta},\\widehat{e}\\right\\rangle =\\widehat{\\beta}'X'\\widehat{e}=0_{K}^{\\prime}$, 其中$\\left\\langle \\cdot,\\cdot\\right\\rangle$ 是向量的内积运算. 那么, $X\\widehat{\\beta}$ 就是 $Y$ 在 $\\mathcal{X}$ 上的*投影 (projection)* , $\\widehat{e}$ 则是对应的*投影残差 (projection\n",
    "residuals)*. 根据勾股定理, 自然有\n",
    "\n",
    "$$\n",
    "\\left\\Vert Y\\right\\Vert ^{2}=\\Vert X\\widehat{\\beta}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:example}\n",
    ":label: example1\n",
    "\n",
    "下面是一个简单的数值模拟案例, 我们以此来说明最小二乘法估计量的特性. 给定\n",
    "$\\left(x_{1i},x_{2i},x_{3i},e_{i}\\right)^{\\prime}\\sim N\\left(0_{4},I_{4}\\right)$, 因变量 $y_{i}$ 的生成式为\n",
    "$$\n",
    "y_{i}=0.5+2\\cdot x_{1i}-1\\cdot x_{2i}+e_{i}\n",
    "$$\n",
    "\n",
    "在不知道 $x_{3i}$ 是多余的情况下, 我们将 $y_{i}$ 对\n",
    "$\\left(1,x_{1i},x_{2i},x_{3i}\\right)$ 进行回归. \n",
    "\n",
    "```r\n",
    "library(magrittr); set.seed(2022-6-15)\n",
    "n = 20 # sample size \n",
    "K = 4 # number of paramters\n",
    "b0 = as.matrix( c(0.5, 2, -1, 0) ) # the true coefficient\n",
    "X = cbind(1, matrix( rnorm(n * (K-1)), nrow = n ) ) # the regressor matrix \n",
    "e = rnorm(n) # the error term\n",
    "Y = X %*% b0 + e # generate the dependent variable\n",
    "bhat = solve(t(X) %*% X, t(X) %*% Y ) %>% as.vector() %>% print()\n",
    "```\n",
    "\n",
    "最后得到的参数估计值与真实值已经很接近. 当然, 由于样本容量较小, 答案不是十分准确. \n",
    "\n",
    "```r\n",
    "ehat = Y - X %*% bhat \n",
    "as.vector( t(X) %*% ehat ) %>% print()\n",
    "MX = diag(n) - X %*% solve( crossprod(X) ) %*% t(X)\n",
    "data.frame(e = e, ehat = ehat, MXY = MX%*%Y, MXe = MX%*%e ) %>% head()\n",
    "cat(\"The mean of the residual is \", mean(ehat), \".\\n\")\n",
    "cat(\"The mean of the true error term is\", mean(e), \".\")\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子向量 (Subvector) \n",
    "\n",
    "根据 Frish-Waugh-Lovell (FWL) 定理, 最小二乘法估计量的子向量有一个特殊的代数性质. 要得到FWL定理, 我们引入分块矩阵的逆矩阵这一概念. 对于一个实对称的正定矩阵\n",
    "$A=\\begin{pmatrix}A_{11} & A_{12}\\\\\n",
    "A_{12}' & A_{22}\n",
    "\\end{pmatrix},$\n",
    "它的逆矩阵可以写作\n",
    "\n",
    "$$\n",
    "A^{-1}=\\begin{pmatrix}\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & -\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1}A_{12}A_{22}^{-1}\\\\\n",
    "-A_{22}^{-1}A_{12}'\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & \\left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\\right)^{-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "将此性质运用至最小二乘法估计量. 记 $X=\\left(\\begin{array}{cc}\n",
    "X_{1} & X_{2}\\end{array}\\right),$ 那么\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{pmatrix}\\widehat{\\beta}_{1}\\\\\n",
    "\\widehat{\\beta}_{2}\n",
    "\\end{pmatrix} & =\\widehat{\\beta}=(X'X)^{-1}X'Y\\\\\n",
    " & =\\left(\\begin{pmatrix}X_{1}'\\\\\n",
    "X_{2}'\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1} & X_{2}\\end{pmatrix}\\right)^{-1}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\\\\n",
    "X_{2}'X_{1} & X_{2}'X_{2}\n",
    "\\end{pmatrix}^{-1}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1} & -\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}\\\\\n",
    "\\heartsuit & \\heartsuit\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}.\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\widehat{\\beta}$ 的子向量可写作 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\beta}_{1} & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}X_{2}'Y\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'P_{X_{2}}Y\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}\\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\\right)\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'M_{X_{2}}Y.\\end{aligned}\n",
    "$$\n",
    "\n",
    "注意, 我们可以用三步方法得到 $\\widehat{\\beta}_{1}$: \n",
    "\n",
    "1. 将 $Y$ 对 $X_{2}$ 做回归, 得到残差 $\\tilde{Y}$;\n",
    "\n",
    "2. 将 $X_{1}$ 对 $X_{2}$ 做回归, 得到残差 $\\tilde{X}_{1}$;\n",
    "\n",
    "3. 将 $\\tilde{Y}$ 对 $\\tilde{X}_{1}$ 做回归, 得到最小二乘法估计值 $\\widehat{\\beta}_{1}$.\n",
    "\n",
    "同样的方法也适用于总体线性投影 (population linear\n",
    "projection) , 参考Hansen (2020) \\[E\\] Chapter 2.22-23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X1 = X[,1:2];X2 = X[,3:4]\n",
    "PX2 = X2 %*% solve( t(X2) %*% X2) %*% t(X2) \n",
    "MX2 = diag(rep(1,n)) - PX2\n",
    "\n",
    "bhat1 <- (solve(t(X1)%*% MX2 %*% X1, t(X1) %*% MX2 %*% Y )) %>%\n",
    " as.vector() %>% print()\n",
    "\n",
    "ehat1 = MX2 %*% Y - MX2 %*% X1 %*% bhat1 \n",
    "data.frame(ehat = ehat, ehat1 = ehat1) %>% head() %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适配度检验 (Goodness of Fit) \n",
    "\n",
    "考虑回归方程\n",
    "$Y=X_{1}\\beta_{1}+\\beta_{2}+e$. 代入最小二乘法估计量, 我们得到\n",
    "\n",
    "$$\n",
    "Y=\\widehat{Y}+\\widehat{e}=\\left(X_{1}\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}\\right)+\\widehat{e}\n",
    "$$(eqn:2)\n",
    "\n",
    "应用FWL定理, 令 $X_{2}=\\iota$, 其中希腊字母 $\\iota$ (iota)是一个所有位置都为1的 $n\\times1$ 向量. 那么$M_{X_{2}}=M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota'$. 其中, $M_{\\iota}z=z-\\bar{z}$. 也就是说, 我们在原本的向量 $z$ 中减去其均值 $\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$. \n",
    "\n",
    "那么, 上述的三步方法变为\n",
    "\n",
    "1. 将 $Y$ 对 $\\iota$ 做回归, 得到残差 $M_{\\iota}Y$;\n",
    "\n",
    "2. 将 $X_{1}$ 对 $\\iota$ 做回归, 得到残差 $M_{\\iota}X_{1}$;\n",
    "\n",
    "3. 将 $M_{\\iota}Y$ 对 $M_{\\iota}X_{1}$ 做回归, 得到最小二乘法估计值 $\\widehat{\\beta}_{1}$ ——与 $(3.2)$ 中得到的结果完全一致. \n",
    "\n",
    "\n",
    "我们将最后一步进行分解\n",
    "\n",
    "$$\n",
    "M_{\\iota}Y=M_{\\iota}X_{1}\\widehat{\\beta}_{1}+\\tilde{e},\n",
    "$$(eqn:3)\n",
    "\n",
    "应用勾股定理得到\n",
    "\n",
    "$$\n",
    "\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}=\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: exercise31\n",
    "\n",
    "试说明: {eq}`eqn:2` 式中的 $\\widehat{e}$ 与 {eq}`eqn:3` 式中的 $\\tilde{e}$ 相等. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中, *决定系数* ($R^2$) 是一个衡量适配度的指标. 样本内决定系数(in-sample $R^2$)可写作\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}=1-\\frac{\\left\\Vert \\tilde{e}\\right\\Vert ^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}.\n",
    "$$\n",
    "\n",
    "仅当回归因子包含常数时, $R^2$ 才有定义. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: exercise32\n",
    "\n",
    "在 {eq}`eqn:2` 式的分解下, 证明\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\widehat{Y}'M_{\\iota}\\widehat{Y}}{Y'M_{\\iota}Y}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y_{i}}-\\overline{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}}.\n",
    "$$\n",
    "注意, 上述 $R^{2}$ 即为$\\widehat{Y}$的样本方差与 $Y$ 的样本方差之比值. \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "决定系数 $R^2$ 的大小在不同的实际问题下差别很大. 在具有滞后效应的宏观模型中, 遇到 $R^2$ 大于90%的情况并不罕见. 然而, 在横向研究 (cross sectional regressions) 中, $R^2$ 的值经常是低于20%的. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: exercise33\n",
    "\n",
    "考虑一个较“短”的回归 \"将 $y_{i}$ 对 $x_{1i}$ 做回归\" 与一个较“长”的回归\n",
    " \"将 $y_{i}$ 对 $\\left(x_{1i},x_{2i}\\right)$ 做回归\": 给定相同的数据集 $\\left(Y,X_{1},X_{2}\\right)$, 试说明“短”回归的 $R^2$ 不比“长”回归的 $R^2$ 大. (也就是说, 通过增加更多的回归因子, 我们总能使得 $R^2$ 增大或者不变. ) \n",
    "```\n",
    "\n",
    "在传统意义上, 回归因子的数量 $K$ 总是远小于样本量 $n$ . 然而, 在大数据时代, (潜在) 回归因子的数量 $K$ 有可能大于样本量 $n$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{exercise}\n",
    ":label: exercise34\n",
    "\n",
    "证明: 当 $K\\geq n$ 时, $R^{2}=1$. 注意, 若 $K>n$, 则矩阵 $X'X$ 是缺秩 (rank deficient) 的. 在这种情况下, 我们可以将最小二乘法的定义扩展, $\\hat \\beta$ 仍然是使得 $\\left\\Vert Y-Xb\\right\\Vert ^{2}$ 最小化的参数值, 但这个值不是唯一的. \n",
    "\n",
    "```r\n",
    "n = 5; K = 6; \n",
    "Y = rnorm(n)\n",
    "X = matrix( rnorm(n*K), n)\n",
    "summary( lm(Y~X) )\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个新的数据集 $\\left(Y^{\\mathrm{new}},X^{\\mathrm{new}}\\right)$, 样本外决定系数(OOS $R^2$) 可写作\n",
    "\n",
    "$$\n",
    "OOS\\ R^{2}=\\frac{\\widehat{\\beta}^{\\prime}X^{\\mathrm{new}\\prime}M_{\\iota}X^{\\mathrm{new}}\\widehat{\\beta}}{Y^{\\mathrm{new}\\prime}M_{\\iota}Y^{\\mathrm{new}}}.\n",
    "$$\n",
    "\n",
    "从原始数据集中得到参数估计值以后, $OOS\\ R^{2}$ 衡量这一参数估计值在新数据集上的适配度. 在金融市场的短期预测模型中, 如果某策略能够系统性地获得2\\%的 $OOS\\ R^{2}$, 那么这就是一个很好的赚钱策略. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 总结\n",
    "\n",
    "以上有关线性代数的性质, 在有限样本中是成立的, 无论数据是固定数字或是随机变量. 然而高斯马尔科夫定理 (Gauss Markov theorem) 只在两个关键假设下成立: 线性的条件期望方程 (linear CEF) 和同方差性 (homoscedasticity) . \n",
    "\n",
    "\n",
    "```{admonition} 历史趣闻\n",
    "Carl Friedrich Gauss (1777--1855) 宣称他在1795年就发明了最小二乘法, 那时他用三个数据点预测了1801年矮行星谷神星的位置. 虽然 Gauss 在1809年才发表了相关文章, 而 Adrien-Marie Legendre (1752--1833) 在1805年就公开提出了此方法, 但今天人们仍倾向于将最小二乘法的发明归功于 Gauss. 因为像 Gauss 这样的数学巨人没有必要通过撒谎来窃取 Legendre 的成果. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
