{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conditional Expectation\n",
                "\n",
                "**Notation**: In this note, $y$ is a scale random variable, and\n",
                "$x=\\left(x_{1},\\ldots,x_{K}\\right)'$ is a $K\\times1$ random vector.\n",
                "Throughout this course, a vector is a *column* vector, i.e. a one-column\n",
                "matrix.\n",
                "\n",
                "\n",
                "-----------------------\n",
                "\n",
                "Machine learning is a big basket that contains the regression models. We\n",
                "motivate the conditional expectation model from the perspective of\n",
                "prediction. We view a regression as *supervised learning*. Supervised\n",
                "learning uses a function of $x$, say, $g\\left(x\\right)$, to predict $y$.\n",
                "$x$ cannot perfectly predict $y$; otherwise their relationship is\n",
                "deterministic. The prediction error $y-g\\left(x\\right)$ depends on the\n",
                "choice of $g$. There are numerous possible choices of $g$. Which one is\n",
                "the best? Notice that this question is not concerned about the\n",
                "underlying data generating process (DGP) of the joint distribution of\n",
                "$\\left(y,x\\right)$. We want to find a general rule to achieve accurate\n",
                "prediction of $y$ given $x$, no matter how this pair of variables is\n",
                "generated.\n",
                "\n",
                "To answer this question, we need to decide a criterion to compare\n",
                "different $g$. Such a criterion is called the *loss function*\n",
                "$L\\left(y,g\\left(x\\right)\\right)$. A particularly convenient one is the\n",
                "*quadratic loss*, defined as\n",
                "$$L\\left(y,g\\left(x\\right)\\right)=\\left(y-g\\left(x\\right)\\right)^{2}.$$\n",
                "Since the data are random, $L\\left(y,g\\left(x\\right)\\right)$ is also\n",
                "random. \"Random\" means uncertainty: sometimes *this* happens, and\n",
                "sometimes *that* happens. To get rid of the uncertainty, we average the\n",
                "loss function with respect to the joint distribution of\n",
                "$\\left(y,x\\right)$ as\n",
                "$R\\left(y,g\\left(x\\right)\\right)=E\\left[L\\left(y,g\\left(x\\right)\\right)\\right]$,\n",
                "which is called *risk*. Risk is a deterministic quality. For the\n",
                "quadratic loss function, the corresponding risk is\n",
                "$$R\\left(y,g\\left(x\\right)\\right)=E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right],$$\n",
                "is called the *mean squared error* (MSE). MSE is the most widely used\n",
                "risk measure, although there exist many alternative measures, for\n",
                "example the *mean absolute error* (MAE)\n",
                "$E\\left[\\left|y-g\\left(x\\right)\\right|\\right]$. The popularity of MSE\n",
                "comes from its convenience for analysis in closed-form, which MAE does\n",
                "not enjoy due to its nondifferentiability. This is similar to the choice\n",
                "of utility functions in economics. There are only a few functional forms\n",
                "for the utility, for example CRRA, CARA, and so on. They are popular\n",
                "because they lead to close-form solutions that are easy to handle. Now\n",
                "our quest is narrowed to: What is the optimal choice of $g$ if we\n",
                "minimize the MSE?\n",
                "\n",
                "[\\[prop:CEF\\]]{#prop:CEF label=\"prop:CEF\"} The conditional mean function\n",
                "(CEF)\n",
                "$m\\left(x\\right)=E\\left[y|x\\right]=\\int yf\\left(y|x\\right)\\mathrm{d}y$\n",
                "minimizes MSE.\n",
                "\n",
                "Before we prove the above proposition, we first discuss some properties\n",
                "of the conditional mean function. Obviously\n",
                "$$y=m\\left(x\\right)+\\left(y-m\\left(x\\right)\\right)=m\\left(x\\right)+\\epsilon,$$\n",
                "where $\\epsilon:=y-m\\left(x\\right)$ is called the *regression error*.\n",
                "This equation holds for $\\left(y,x\\right)$ following any joint\n",
                "distribution, as long as $E\\left[y|x\\right]$ exists. The error term\n",
                "$\\epsilon$ satisfies these properties:\n",
                "\n",
                "-   $E\\left[\\epsilon|x\\right]=E\\left[y-m\\left(x\\right)|x\\right]=E\\left[y|x\\right]-m(x)=0$,\n",
                "\n",
                "-   $E\\left[\\epsilon\\right]=E\\left[E\\left[\\epsilon|x\\right]\\right]=E\\left[0\\right]=0$,\n",
                "\n",
                "-   For any function $h\\left(x\\right)$, we have\n",
                "    $$E\\left[h\\left(x\\right)\\epsilon\\right]=E\\left[E\\left[h\\left(x\\right)\\epsilon|x\\right]\\right]=E\\left[h(x)E\\left[\\epsilon|x\\right]\\right]=0.\\label{eq:uncorr}$$\n",
                "\n",
                "The last property implies that $\\epsilon$ is uncorrelated with any\n",
                "function of $x$. In particular, when $h$ is the identity function\n",
                "$h\\left(x\\right)=x$, we have\n",
                "$E\\left[x\\epsilon\\right]=\\mathrm{cov}\\left(x,\\epsilon\\right)=0$.\n",
                "\n",
                "\\bigskip{}\n",
                "The optimality of the CEF can be confirmed by \"guess-and-verify.\" For an\n",
                "arbitrary $g\\left(x\\right)$, the MSE can be decomposed into three terms\n",
                "$$\\begin{aligned}\n",
                " &  & E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right]\\\\\n",
                " & = & E\\left[\\left(y-m(x)+m(x)-g(x)\\right)^{2}\\right]\\\\\n",
                " & = & E\\left[\\left(y-m\\left(x\\right)\\right)^{2}\\right]+2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]+E\\left[\\left(m\\left(x\\right)-g\\left(x\\right)\\right)^{2}\\right].\\end{aligned}$$\n",
                "The first term is irrelevant to $g\\left(x\\right)$. The second term\n",
                "$$\\begin{aligned}2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right] & =2E\\left[\\epsilon\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]=0\\end{aligned}$$\n",
                "by invoking ([\\[eq:uncorr\\]](#eq:uncorr){reference-type=\"ref\"\n",
                "reference=\"eq:uncorr\"}) with\n",
                "$h\\left(x\\right)=m\\left(x\\right)-g\\left(x\\right)$. The second term is\n",
                "again irrelevant of $g\\left(x\\right)$. The third term, obviously, is\n",
                "minimized at $g\\left(x\\right)=m\\left(x\\right)$.\n",
                "\n",
                "\\bigskip{}\n",
                "Our perspective so far deviates from many econometric textbooks that\n",
                "assume that the dependent variable $y$ is generated as\n",
                "$g\\left(x\\right)+\\epsilon$ for some unknown function\n",
                "$g\\left(\\cdot\\right)$ and error term $\\epsilon$ such that\n",
                "$E\\left[\\epsilon|x\\right]=0$. Instead, we take a predictive approach\n",
                "regardless the DGP. What we observe are $y$ and $x$ and we are solely\n",
                "interested in seeking a function $g\\left(x\\right)$ to predict $y$ as\n",
                "accurately as possible under the MSE criterion.\n",
                "\n",
                "Linear Projection\n",
                "-----------------\n",
                "\n",
                "The CEF $m(x)$ is the function that minimizes the MSE. However,\n",
                "$m\\left(x\\right)=E\\left[y|x\\right]$ is a complex function of $x$, for it\n",
                "depends on the joint distribution of $\\left(y,x\\right)$, which is mostly\n",
                "unknown in practice. Now let us make the prediction task even simpler.\n",
                "How about we minimize the MSE within all linear functions in the form of\n",
                "$h\\left(x\\right)=h\\left(x;b\\right)=x'b$ for $b\\in\\mathbb{R}^{K}$? The\n",
                "minimization problem is\n",
                "$$\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x'b\\right)^{2}\\right].\\label{eq:linear_MSE}$$\n",
                "Take the first-order condition of the MSE\n",
                "$$\\frac{\\partial}{\\partial b}E\\left[\\left(y-x'b\\right)^{2}\\right]=E\\left[\\frac{\\partial}{\\partial b}\\left(y-x'b\\right)^{2}\\right]=-2E\\left[x\\left(y-x'b\\right)\\right],$$\n",
                "where the first equality holds if\n",
                "$E\\left[\\left(y-x'b\\right)^{2}\\right]<\\infty$ so that the expectation\n",
                "and partial differentiation is interchangeable, and the second equality\n",
                "hods by the chain rule and the linearity of expectation. Set the first\n",
                "order condition to 0 and we solve\n",
                "$$\\beta=\\arg\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x'b\\right)^{2}\\right]$$\n",
                "in the closed-form\n",
                "$$\\beta=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right]$$ if\n",
                "$E\\left[xx'\\right]$ is invertible. Notice here that $b$ is an arbitrary\n",
                "$K$-vector, while $\\beta$ is the optimizer. The function $x'\\beta$ is\n",
                "called the *best linear projection* (BLP) of $y$ on $x$, and the vector\n",
                "$\\beta$ is called the *linear projection coefficient*.\n",
                "\n",
                "\\bigskip{}\n",
                "The linear function is not as restrictive as one might thought. It can\n",
                "be used to produce some nonlinear (in random variables) effect if we\n",
                "re-define $x$. For example, if\n",
                "$$y=x_{1}\\beta_{1}+x_{2}\\beta_{2}+x_{1}^{2}\\beta_{3}+e,$$ then\n",
                "$\\frac{\\partial}{\\partial x_{1}}m\\left(x_{1},x_{2}\\right)=\\beta_{1}+2x_{1}\\beta_{3}$,\n",
                "which is nonlinear in $x_{1}$, while it is still linear in the parameter\n",
                "$\\beta=\\left(\\beta_{1},\\beta_{2},\\beta_{3}\\right)$ if we define a set of\n",
                "new regressors as\n",
                "$\\left(\\tilde{x}_{1},\\tilde{x}_{2},\\tilde{x}_{3}\\right)=\\left(x_{1},x_{2},x_{1}^{2}\\right)$.\n",
                "\n",
                "If $\\left(y,x\\right)$ is jointly normal in the form $$\\begin{pmatrix}y\\\\\n",
                "x\n",
                "\\end{pmatrix}\\sim\\mathrm{N}\\left(\\begin{pmatrix}\\mu_{y}\\\\\n",
                "\\mu_{x}\n",
                "\\end{pmatrix},\\begin{pmatrix}\\sigma_{y}^{2} & \\rho\\sigma_{y}\\sigma_{x}\\\\\n",
                "\\rho\\sigma_{y}\\sigma_{x} & \\sigma_{x}^{2}\n",
                "\\end{pmatrix}\\right)$$ where $\\rho$ is the correlation coefficient, then\n",
                "$$E\\left[y|x\\right]=\\mu_{y}+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\left(x-\\mu_{x}\\right)=\\left(\\mu_{y}-\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\mu_{x}\\right)+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}x,$$\n",
                "is a liner function of $x$. In this example, the CEF is linear.\n",
                "\n",
                "Even though in general $m\\left(x\\right)\\neq x'\\beta$, the linear form\n",
                "$x'\\beta$ is still useful in approximating $m\\left(x\\right)$. That is,\n",
                "$\\beta=\\arg\\min\\limits _{b\\in\\mathbb{R}^{K}}E\\left[\\left(m(x)-x'b\\right)^{2}\\right]$.\n",
                "\n",
                "The first-order condition gives\n",
                "$\\frac{\\partial}{\\partial b}E\\left[\\left(m(x)-x'b\\right)^{2}\\right]=-2E[x(m(x)-x'b)]=0$.\n",
                "Rearrange the terms and obtain $E[x\\cdot m(x)]=E[xx']b$. When $E[xx']$\n",
                "is invertible, we solve\n",
                "$$\\left(E\\left[xx'\\right]\\right){}^{-1}E[x\\cdot m(x)]=\\left(E\\left[xx'\\right]\\right){}^{-1}E[E[xy|x]]=\\left(E\\left[xx'\\right]\\right){}^{-1}E[xy]=\\beta.$$\n",
                "Thus $\\beta$ is also the best linear approximation to $m\\left(x\\right)$\n",
                "under MSE.\n",
                "\n",
                "\\bigskip{}\n",
                "We may rewrite the linear regression model, or the *linear projection\n",
                "model,* as $$\\begin{array}[t]{c}\n",
                "y=x'\\beta+e\\\\\n",
                "E[xe]=0,\n",
                "\\end{array}$$ where $e=y-x'\\beta$ is called the *linear projection\n",
                "error*, to be distinguished from $\\epsilon=y-m(x).$\n",
                "\n",
                "Show (a) $E\\left[xe\\right]=0$. (b) If $x$ contains a constant, then\n",
                "$E\\left[e\\right]=0$.\n",
                "\n",
                "### Omitted Variable Bias\n",
                "\n",
                "We write the *long regression* as\n",
                "$$y=x_{1}'\\beta_{1}+x_{2}'\\beta_{2}+\\beta_{3}+e_{\\beta},$$ and the\n",
                "*short regression* as $$y=x_{1}'\\gamma_{1}+\\gamma_{2}+e_{\\gamma},$$\n",
                "where $e_{\\beta}$ and $e_{\\gamma}$ are the projection errors,\n",
                "respectively. If $\\beta_{1}$ in the long regression is the parameter of\n",
                "interest, omitting $x_{2}$ as in the short regression will render\n",
                "*omitted variable bias* (meaning $\\gamma_{1}\\neq\\beta_{1}$) unless\n",
                "$x_{1}$ and $x_{2}$ are uncorrelated.\n",
                "\n",
                "We first demean all the variables in the two regressions, which is\n",
                "equivalent as if we project out the effect of the constant. The long\n",
                "regression becomes\n",
                "$$\\tilde{y}=\\tilde{x}_{1}'\\beta_{1}+\\tilde{x}_{2}'\\beta_{2}+\\tilde{e}_{\\beta},$$\n",
                "and the short regression becomes\n",
                "$$\\tilde{y}=\\tilde{x}_{1}'\\gamma_{1}+\\tilde{e}_{\\gamma},$$ where *tilde*\n",
                "denotes the demeaned variable.\n",
                "\n",
                "Show $\\tilde{e}_{\\beta}=e_{\\beta}$ and $\\tilde{e}_{\\gamma}=e_{\\gamma}$.\n",
                "\n",
                "After demeaning, the cross-moment equals to the covariance. The short\n",
                "regression coefficient\n",
                "$$\\begin{aligned}\\gamma_{1} & =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{y}\\right]\\\\\n",
                " & =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\left(\\tilde{x}_{1}'\\beta_{1}+\\tilde{x}_{2}'\\beta_{2}+\\tilde{e}_{\\beta}\\right)\\right]\\\\\n",
                " & =\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2}\\\\\n",
                " & =\\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2},\n",
                "\\end{aligned}$$ where the third line holds as\n",
                "$E\\left[\\tilde{x}_{1}\\tilde{e}_{\\beta}\\right]=0$. Therefore,\n",
                "$\\gamma_{1}=\\beta_{1}$ if and only if\n",
                "$E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2}=0$, which demands\n",
                "either $E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]=0$ or $\\beta_{2}=0$.\n",
                "\n",
                "Show that\n",
                "$E\\left[\\left(y-x_{1}'\\beta_{1}-x_{2}'\\beta_{2}-\\beta_{3}\\right)^{2}\\right]\\leq E\\left[\\left(y-x_{1}'\\gamma_{1}-\\gamma_{2}\\right)^{2}\\right]$.\n",
                "\n",
                "Obviously we prefer to run the long regression to attain $\\beta_{1}$ if\n",
                "possible, for it is a more general model than the short regression and\n",
                "achieves no larger variance in the projection error. However, sometimes\n",
                "$x_{2}$ is unobservable so the long regression is unavailable. This\n",
                "example of omitted variable bias is ubiquitous in applied econometrics.\n",
                "Ideally we would like to directly observe some regressors but in reality\n",
                "we do not have them at hand. We should be aware of the potential\n",
                "consequence when the data are not as ideal as we have wished. When only\n",
                "the short regression is available, in some cases we are able to sign the\n",
                "bias, meaning that we can argue whether $\\gamma_{1}$ is bigger or\n",
                "smaller than $\\beta_{1}$ based on our knowledge.\n",
                "\n",
                "Causality\n",
                "---------\n",
                "\n",
                "### Structure and Identification\n",
                "\n",
                "Unlike physical laws such as Einstein's mass--energy equivalence\n",
                "$E=mc^{2}$ and Newton's universal gravitation $F=Gm_{1}m_{2}/r^{2}$,\n",
                "economic phenomena can rarely be summarized in such a minimalistic\n",
                "style. When using experiments to verify physical laws, scientists often\n",
                "manage to come up with smart design in which signal-to-noise ratio is so\n",
                "high that small disturbances are kept at a negligible level. On the\n",
                "contrary, economic laws do not fit a laboratory for experimentation.\n",
                "What is worse, the subjects in economic studies --- human beings --- are\n",
                "heterogeneous and with many features that are hard to control. People\n",
                "from distinctive cultural and family backgrounds respond to the same\n",
                "issue differently and researchers can do little to homogenize them. The\n",
                "signal-to-noise ratios in economic laws are often significantly lower\n",
                "than those of physical laws, mainly due to the lack of laboratory\n",
                "setting and the heterogeneous nature of the subjects.\n",
                "\n",
                "Educational return and the demand-supply system are two classical topics\n",
                "in econometrics. A person's incomes is determined by too many random\n",
                "factors in the academic and career path that is impossible to\n",
                "exhaustively observe and control. The observable prices and quantities\n",
                "are outcomes of equilibrium so the demand and supply affect each other.\n",
                "\n",
                "Generations of thinkers have been debating the definitions of causality.\n",
                "In economics, an accepted definition is *structural causality*.\n",
                "Structural causality is a thought experiment. It assumes that there is a\n",
                "DGP that produces the observational data. If we can use data to recover\n",
                "the DGP or some features of the DGP, then we have learned causality or\n",
                "some implications of causality.\n",
                "\n",
                "A key issue to resolve before looking at the realized sample is\n",
                "*identification*. We say a model or DGP is *identified* if the each\n",
                "possible parameter of the model under consideration generates\n",
                "distinctive features of the observable data. A model is\n",
                "*under-identified* if more than one parameter in the model can generate\n",
                "exact the same features of the observable data. In other words, a model\n",
                "is under-identified if from the observable data we cannot trace back to\n",
                "a unique parameter in the model. A correctly specified model is the\n",
                "prerequisite for any discussion of identification. In reality, all\n",
                "models are wrong. Thus when talking about identification, we are\n",
                "indulged in an imaginary world. If in such a thought experiment we still\n",
                "cannot unique distinguish the true parameter of the data generating\n",
                "process, then identification fails. We cannot determine what is the true\n",
                "model no matter how large the sample is.\n",
                "\n",
                "### Treatment Effect\n",
                "\n",
                "We narrow down to the framework of the relationship between $y$ and $x$.\n",
                "One question of particular interest is *treatment effect*. The treatment\n",
                "effect is how much $y$ will change if we change a variable of interest,\n",
                "say $d$, by one unit while keeping all other variables (including the\n",
                "unobservable variables) the same. The Latin phrase *ceteris paribus*\n",
                "means \"keep all other things constant.\"\n",
                "\n",
                "During the 2020 covid-19 pandemic, Hong Kong's unemployment rate rose to\n",
                "a high-level and consumption collapsed. In order to boost the economy,\n",
                "some Hong Kong residents were qualified in receiving 10,000 HKD cash\n",
                "allowance from the government. We are interested in learning how much\n",
                "does the 10,000 HKD allowance increase people's consumption. For an\n",
                "individual, we imagine two parallel worlds: one with the cash allowance\n",
                "and one without. The difference of the consumption in the world with the\n",
                "allowance, denoted $Y\\left(1\\right)$, and that in the world without the\n",
                "allowance, denoted $Y\\left(0\\right)$, is the treatment effect of that\n",
                "particular person. This thought experiment is called the *potential\n",
                "outcome framework*.\n",
                "\n",
                "However, in reality one and only one scenario happens, which echoes the\n",
                "saying of ancient Greek philosopher Heraclitus (553 BC\\--475 BC) \"You\n",
                "cannot step into the same river twice.\" The individual treatment effect\n",
                "is not operational (*operational* means it can be computed from data at\n",
                "the population level), because one and only one outcome is realized.\n",
                "With many people available, we can define *average treatment effect*\n",
                "(ATE) as\n",
                "$$ATE=E\\left[Y\\left(1\\right)-Y\\left(0\\right)\\right]=E\\left[Y\\left(1\\right)\\right]-E\\left[Y\\left(0\\right)\\right].$$\n",
                "Notice that $E\\left[Y\\left(1\\right)\\right]$ and\n",
                "$E\\left[Y\\left(0\\right)\\right]$ are still not operational before we\n",
                "observe a companion variable\n",
                "$$D=1\\left\\{ \\mbox{treatment received}\\right\\} .$$ Once each\n",
                "individual's treatment status is observable,\n",
                "$E\\left[Y\\left(1\\right)|D=1\\right]$ and\n",
                "$E\\left[Y\\left(0\\right)|D=0\\right]$ are operational from the data.\n",
                "\n",
                "If the two potential outcomes\n",
                "$\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)$ are independent of the\n",
                "assignment $D$, then\n",
                "$E\\left[Y\\left(1\\right)\\right]=E\\left[Y\\left(1\\right)|D=1\\right]$ and\n",
                "$E\\left[Y\\left(0\\right)\\right]=E\\left[Y\\left(0\\right)|D=0\\right]$ so\n",
                "that ATE can be estimated from the data in an operational way as\n",
                "$$ATE=E\\left[Y\\left(1\\right)|D=1\\right]-E\\left[Y\\left(0\\right)|D=0\\right].$$\n",
                "Therefore, to evaluate ATE ideally we would like use a lottery to\n",
                "randomly decide that some people receive the treatment (*treatment\n",
                "group*, with $D=1$) and the others do not (*control group*, with $D=0$).\n",
                "\n",
                "When we have other control variables, we can also define a finer\n",
                "treatment effect conditional on $x$:\n",
                "$$ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|x\\right]-E\\left[Y\\left(0\\right)|x\\right].$$\n",
                "ATE is the average effect in the population of individuals when we\n",
                "hypothetical give them the treatment, keeping all other factors $x$\n",
                "constant. If conditioning on $x$, the treatment $D$ is independent of\n",
                "$\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)$, then ATE becomes\n",
                "operational:\n",
                "$$ATE\\left(x\\right)=E\\left[Y\\left(1\\right)|D=1,x\\right]-E\\left[Y\\left(0\\right)|D=0,x\\right]$$\n",
                "The important condition\n",
                "$\\left(\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)\\perp D\\right)|x$ is\n",
                "called the *conditional independence assumption* (CIA).\n",
                "\n",
                "CIA is more plausible than full independence. Consider the example\n",
                "$Y\\left(1\\right)=x+u\\left(1\\right)$, $Y\\left(0\\right)=x+u\\left(0\\right)$\n",
                "and $D=1\\left\\{ x+u_{d}\\geq0\\right\\}$. If\n",
                "$\\left(\\left(u\\left(0\\right),u\\left(1\\right)\\right)\\perp u_{d}\\right)|x$,\n",
                "then CIA is satisfied. Nevertheless\n",
                "$\\left(Y\\left(1\\right),Y\\left(0\\right)\\right)$ and $D$ are statistically\n",
                "dependent, since $x$ is involved in all random variables.\n",
                "\n",
                "### ATE and CEF\n",
                "\n",
                "In the previous section the treatment $D$ is binary. Now we consider a\n",
                "continuous treatment $D$. Suppose the DGP, or the structural model, is\n",
                "$Y=h\\left(D,x,u\\right)$ where $D$ and $x$ are observable and $u$ is\n",
                "unobservable. It is natural to define ATE with the continuous treatment\n",
                "(Hansen's book Chapter 2.30 calls it *average causal effect*) as\n",
                "$$ATE\\left(d,x\\right)=E\\left[\\lim_{\\Delta\\to0}\\frac{h\\left(d+\\Delta,x,u\\right)-h\\left(d,x,u\\right)}{\\Delta}\\right]=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right],$$\n",
                "where the continuous differentiability of $h\\left(d,x,u\\right)$ at $d$\n",
                "is implicitly assumed. Unlike the binary treatment case, here $d$\n",
                "explicitly shows up in $ATE\\left(d,x\\right)$ because the effect can vary\n",
                "at different values of $d$. ATE here is the average effect in the\n",
                "population of individuals if we hypothetical move $D$ a tiny bit around\n",
                "$d$, keeping all other factors $x$ constant.\n",
                "\n",
                "In the previous sections, we focused on the CEF $m\\left(d,x\\right)$,\n",
                "where $d$ is added to $x$ as an additional variable of interest. We did\n",
                "not intend to model the underlying economic mechanism\n",
                "$h\\left(D,x,u\\right)$, which may be very complex. Can we learn the\n",
                "$ATE\\left(d,x\\right)$ which bears the structural causal interpretation,\n",
                "from the mechanical $m\\left(d,x\\right)$ which merely cares about best\n",
                "prediction? The answer is positive under CIA: $\\left(u\\perp D\\right)|x$.\n",
                "$$\\begin{aligned}\n",
                "\\frac{\\partial}{\\partial d}m\\left(d,x\\right) & =\\frac{\\partial}{\\partial d}E\\left[y|d,x\\right]=\\frac{\\partial}{\\partial d}E\\left[h\\left(d,x,u\\right)|d,x\\right]=\\frac{\\partial}{\\partial d}\\int h\\left(d,x,u\\right)f\\left(u|d,x\\right)du\\\\\n",
                " & =\\int\\frac{\\partial}{\\partial d}\\left[h\\left(d,x,u\\right)f\\left(u|d,x\\right)\\right]du\\\\\n",
                " & =\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du+\\int h\\left(d,x,u\\right)\\left[\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)\\right]du,\\end{aligned}$$\n",
                "where the second line implicitly assumes interchangeability between the\n",
                "integral and the partial derivative. Under CIA,\n",
                "$\\frac{\\partial}{\\partial d}f\\left(u|d,x\\right)=0$ and the second term\n",
                "drops out. Thus\n",
                "$$\\frac{\\partial}{\\partial d}m\\left(d,x\\right)=\\int\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]f\\left(u|d,x\\right)du=E\\left[\\frac{\\partial}{\\partial d}h\\left(d,x,u\\right)\\right]=ATE\\left(d,x\\right).$$\n",
                "This is an important result. It says that if CIA holds, we can learn the\n",
                "causal effect of $d$ on $y$ by the partial derivative of CEF conditional\n",
                "on $x$. In particular, if we further assume a linear CEF\n",
                "$m\\left(d,x\\right)=\\beta_{d}d+\\beta_{x}'x$, then the causal effect is\n",
                "the coefficient $\\beta_{d}$.\n",
                "\n",
                "CIA is the key condition that links the CEF and the causal effect. CIA\n",
                "is not an innocuous assumption. In applications, our causal results are\n",
                "credible only when we can convincing defend CIA.\n",
                "\n",
                "Let factories' output be a Cobb-Douglas function\n",
                "$Y=AK^{\\alpha}L^{\\beta}$, where the capital level $K$ and labor $L$ as\n",
                "well as the output $Y$ is observable, while the \"technology\" $A$ is\n",
                "unobservable. Take logarithm on both sides of the equation:\n",
                "$$y=u+\\alpha k+\\beta l\\label{eq:causal}$$ where $y=\\log Y$, $u=\\log A$,\n",
                "$k=\\log K$ and $l=\\log L$. Suppose $\\begin{pmatrix}u\\\\\n",
                "k\\\\\n",
                "l\n",
                "\\end{pmatrix}\\sim N\\left(\\begin{pmatrix}1\\\\\n",
                "1\\\\\n",
                "1\n",
                "\\end{pmatrix},\\begin{pmatrix}1 & 0.5 & 0\\\\\n",
                "0.5 & 1 & 0\\\\\n",
                "0 & 0 & 1\n",
                "\\end{pmatrix}\\right)$ and $\\alpha=\\beta=1/2$ make the true DGP. Here $u$\n",
                "and $k$ are correlated, because factories of larger scale can afford\n",
                "robots to facilitate automation.\n",
                "\n",
                "1.  What is the partial derivative of CEF when we use $k$ as a treatment\n",
                "    variable for a fixed labor level $l$? (Hint: the CEF is a linear\n",
                "    function thanks to the joint normality.)\n",
                "\n",
                "2.  Does it coincide with $\\alpha=1/2$, the coefficient in the causal\n",
                "    model ([\\[eq:causal\\]](#eq:causal){reference-type=\"ref\"\n",
                "    reference=\"eq:causal\"})? (Hint: No, because CIA is violated.)\n",
                "\n",
                "Sometimes applied researchers assume by brute force that\n",
                "$y=m\\left(d,x\\right)+u$ is the DGP and $E\\left[u|d,x\\right]=0$, where\n",
                "$d$ is the variable of interest and $x$ is the vector of other control\n",
                "variables. Under these assumptions,\n",
                "$$ATE\\left(d,x\\right)=E\\left[\\frac{\\partial}{\\partial d}\\left(m\\left(d,x\\right)+u\\right)|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d}+\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=\\frac{\\partial m\\left(d,x\\right)}{\\partial d},$$\n",
                "where the second equality holds if\n",
                "$\\frac{\\partial}{\\partial d}E\\left[u|d,x\\right]=E\\left[\\frac{\\partial}{\\partial d}u|d,x\\right]$.\n",
                "At a first glance, it seems that the mean independence assumption\n",
                "$E\\left[u|d,x\\right]=0$, which is weaker than CIA, implies the\n",
                "equivalence between $ATE\\left(d,x\\right)$ and\n",
                "$\\partial m\\left(d,x\\right)/\\partial d$ here. However, such slight\n",
                "weakening is achieved by a very strong assumption that the DGP\n",
                "$h\\left(d,x,u\\right)$ follows the additive separable form\n",
                "$m\\left(d,x\\right)+u$. Without economic theory to defend the choice of\n",
                "the assumed DGP $y=m\\left(d,x\\right)+u$, this is at best the\n",
                "*reduced-form* approach.\n",
                "\n",
                "The *structural approach* here models the economic mechanism, guided by\n",
                "economic theory. The *reduced-form approach* is convenient and can\n",
                "document stylized facts when suitable economic theory is not immediately\n",
                "available. There are constant debates about the pros and cons of the two\n",
                "approaches; see *Journal of Economic Perspectives* Vol. 24, No. 2 Spring\n",
                "2010. In macroeconomics, the so-called Phillips curve, attributed to\n",
                "A.W. Phillips about the negative correlation between inflation and\n",
                "unemployment, is a stylized fact learned from the reduced-form approach.\n",
                "The Lucas critique [@lucas1976econometric] exposed its lack of\n",
                "microfoundation and advocated modeling deep parameters that are\n",
                "invariant to policy changes. The latter is a structural approach.\n",
                "Ironically, more than 40 years has passed since the Lucas critique,\n",
                "equations with little microfoundation still dominate the analytical\n",
                "apparatus of central bankers.\n",
                "\n",
                "Summary\n",
                "-------\n",
                "\n",
                "In this lecture, we cover the conditional mean function and causality.\n",
                "When we are faced with a pair of random variable $\\left(y,x\\right)$\n",
                "drawn from some joint distribution, the CEF is the best predictor. When\n",
                "we go further into the structural causality about some treatment $d$ to\n",
                "the dependent variable $y$, under CIA we can find equivalence between\n",
                "ATE and the partial derivative of CEF. All analyses are conducted in\n",
                "population. We have not touched the sample yet.\n",
                "\n",
                "**Historical notes**: Regressions and conditional expectations are\n",
                "concepts from statistics and they are imported to econometrics in early\n",
                "time. Researchers at the Cowles Commission (now Cowles Foundation for\n",
                "Research in Economics) --- Jacob Marschak (1898--1977), Tjalling\n",
                "Koopmans (1910--1985, Nobel Prize 1975), Trygve Haavelmo (1911--1999,\n",
                "Nobel Prize 1989) and their colleagues --- were trailblazers of the\n",
                "econometric structural approach.\n",
                "\n",
                "The potential outcome framework is not peculiar to economics. It is\n",
                "widely used in other fields such as biostatistics and medical studies.\n",
                "It was initiated by Jerzy Neyman (1894--1981) and extended by Donald\n",
                "B. Rubin (1943-- ), Professor of Statistics at Tsinghua University.\n",
                "\n",
                "**Further reading**: @lewbel2019identification offers a comprehensive\n",
                "summary of identification in econometrics. Accounting is an applied\n",
                "field with many claimed causal inference drawn from simple regressions;\n",
                "it is encouraging to hear @gow2016causal to reflect causality in their\n",
                "practices.\n",
                "\n",
                "\\bigskip\n",
                "` Zhentao Shi. Sep 17, 2020`\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
