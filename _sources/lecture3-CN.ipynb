{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法:线性代数观点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} 《九章算术》第八章：方程\n",
    "今有卖牛二、羊五，以买十三豕，有馀钱一千。卖牛三、豕三，以买九羊，钱适足。卖羊六、豕八，以买五牛，钱不足六百。问牛、羊、豕价各几何?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\\begin{array}{r} \n",
    "2 & 5 & -13\\\\\n",
    "3 & -9 & 3\\\\\n",
    "-5 & 6 & 8\n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c}\n",
    "牛\\\\\n",
    "羊\\\\\n",
    "猪\n",
    "\\end{array} \\right]\n",
    "=\n",
    "\\left[\\begin{array}{r}\n",
    "1000\\\\\n",
    "0\\\\\n",
    "-600\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "A = matrix(c(2, 3, -5, 5, -9, 6, -13, 3, 8), nrow = 3)\n",
    "b = c(1000, 0, -600)\n",
    "solve(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "最小二乘法 (OLS) 是计量经济学中最基本的估计方法，它简单透明，易于理解。了解最小二乘法，有助于我们研究更复杂的线性估计方法。此外，许多非线性估计量在真实值附近与线性估计量的行为是类似的。在本讲义中，我们将从线性代数的运算讲起，学习一系列的知识。\n",
    "\n",
    "\n",
    "套用Leopold Kronecker的名言“上帝创造了整数，其他都是人的作品”，我想说“高斯创造了最小二乘法，其他都是应用研究者的作品”。在科学界，最小二乘法的受欢迎程度远远超出了人们的想象。然而，最小二乘法是一种纯统计学的方法，或者说是一种监督式机器学习的方法，因此它揭示的是相关关系，而非因果关系。相反，经济理论假设因果关系的存在，然后我们收集数据来检验理论或量化效果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学标记: $y_{i}$ 是标量。$x_{i}=\\left(x_{i1},\\ldots,x_{iK}\\right)'$ 是 $K\\times1$ 的向量。$Y=\\left(y_{1},\\ldots,y_{n}\\right)'$ 是 $ n\\times1$ 的向量。\n",
    "\n",
    "$$ \n",
    "X=\\left[\\begin{array}{c}\n",
    "x_{1}'\\\\\n",
    "x_{2}'\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}'\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\cdots & x_{1K}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2K}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{22} & \\cdots & x_{nK}\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "\n",
    "是 $n\\times K$ 的矩阵。$I_{n}$ 是 $n\\times n$ 的单位矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘法\n",
    "\n",
    "给定 $n$ 个观测值 $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$ ， 我们想利用 $X$ 的某个线性组合 $Xb$ 尽量逼近 $Y$。说到逼近，我们需要定义两个向量 $Y$ 和 $Xb$ 之间的距离。欧几里得范数(Euclidean norm) $\\Vert Y - Xb \\Vert = \\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}}$ 是常用的测量向量长度的指标。\n",
    "定义一个目标函数\n",
    "\n",
    "$$\n",
    "Q\\left(b\\right)=\\frac{1}{2n}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}=\\frac{1}{2n}\\left(Y-Xb\\right)'\\left(Y-Xb\\right)=\\frac{1}{2n}\\left\\Vert Y-Xb\\right\\Vert ^{2}.\n",
    "$$\n",
    "\n",
    "**残差平方和** (sum of squared residuals) $\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}$ 是 $\\Vert Y - Xb \\Vert$ 的平方。\n",
    "因为平方 $(\\cdot)^2$ 在 $\\mathbb{R}^+$ 上严格单调递增，所以用 $b$ 来最小化 $Q(b)$ 等价于最小化欧氏距离。系数 $\\frac{1}{2n}$ 与 $b$ 无关，不影响该最优化问题的解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "最小化 $Q\\left(b\\right)$ 的一阶必要条件是\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b}Q\\left(b\\right)=\\left[\\begin{array}{c}\n",
    "\\partial Q\\left(b\\right)/\\partial b_{1}\\\\\n",
    "\\partial Q\\left(b\\right)/\\partial b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\partial Q\\left(b\\right)/\\partial b_{K}\n",
    "\\end{array}\\right]=-\\frac{1}{n}X'\\left(Y-Xb\\right)=0.\n",
    "$$\n",
    "\n",
    "对上式移项可得\n",
    "\n",
    "$$\n",
    "(X'X)b = X'Y.\n",
    "$$\n",
    "\n",
    "在最小二乘法的推导中， 我们假定 $X$ 中的 $K$ 列是**线性独立的**， 也就是说不存在 $K\\times1$ 的向量\n",
    "$b\\ (b\\neq0_{K})$ 使得 $Xb=0_{n}$ 。同时，这也意味着 $n\\geq K$ ，并且 $X'X/n$ 可逆。\n",
    "我们记该最优解 \n",
    "\n",
    "$$\n",
    "\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y.\n",
    "$$ \n",
    "\n",
    "另外，二阶条件\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2}}{\\partial b\\partial b'}Q\\left(b\\right)=\\left[\\begin{array}{cccc}\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}^{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{1}}Q\\left(b\\right)\\\\\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}^{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{2}}Q\\left(b\\right)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{K}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{K}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}^{2}}Q\\left(b\\right)\n",
    "\\end{array}\\right]=\\frac{1}{n}X'X\n",
    "$$\n",
    "\n",
    "表明 $Q\\left(b\\right)$ 是关于 $b$ 的凸函数，因为 $X'X/n$ 是半正定矩阵。(如果 $X'X/n$ 是正定矩阵，那么 $Q\\left(b\\right)$ 是关于 $b$ 的严格凸函数。) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:remark}\n",
    ":label: remark3\n",
    "\n",
    "如果某些回归因子满足**完全共线性** (perfectly collinear)，就违反了 $X'X/n$ 可逆的线性独立的假设。\n",
    "例如，使用虚拟变量来表示分类变量并将所有这些类别放入回归模型中时，通常计量经济学软件会自动检测并指出完全共线性问题。然而，软件难以侦测**不完全共线性** (nearly collinear)，即 $X'X/n$ 的最小特征值接近于0，而不等于0。我们将在渐进理论一章中讨论不完全共线性的后果。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数性质\n",
    "\n",
    "下面列举一些与最小二乘法估计量有关的定义与性质。\n",
    "\n",
    "-  拟合值 (Fitted value): $\\widehat{Y}=X\\widehat{\\beta}$.\n",
    "\n",
    "-  投影矩阵 (Projection matrix): $P_{X}=X\\left(X'X\\right)^{-1}X$ ：残差生成矩阵 (Residual maker\n",
    "  matrix)： $M_{X}=I_{n}-P_{X}$.\n",
    "\n",
    "-  $P_{X}X=X$; $X'P_{X}=X'$.\n",
    "\n",
    "-  $M_{X}X=0_{n\\times K}$; $X'M_{X}=0_{K\\times n}$.\n",
    "\n",
    "-  $P_{X}M_{X}=M_{X}P_{X}=0_{n\\times n}$.\n",
    "\n",
    "-  $P_{X}$ 与 $M_{X}$ 都是 **幂等矩阵** 。\n",
    "  *  如果 $AA=A$ ， 那么 $A$ 被称作 **幂等矩阵** (*idempotent* matrix)。幂等矩阵的特征值只能是1或者0。\n",
    "\n",
    "-  $\\mathrm{rank}\\left(P_{X}\\right)=K$ ， \n",
    "  $\\mathrm{rank}\\left(M_{X}\\right)=n-K$.\n",
    "\n",
    "- 残差: $\\widehat{e}=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y.$\n",
    "\n",
    "-  $X'\\widehat{e}=0_{K}$.\n",
    "\n",
    "-  若 $x_{i}$ 中有一个为常数，则 $\\sum_{i=1}^{n}\\widehat{e}_{i}=0$ 。\n",
    "\n",
    "  (因为 $X'\\widehat{e}=\\left[\\begin{array}{cccc}\n",
    "  1 & 1 & \\cdots & 1\\\\\n",
    "  \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\\\\\n",
    "  \\cdots & \\cdots & \\ddots & \\vdots\\\\\n",
    "  \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\n",
    "  \\end{array}\\right]\\left[\\begin{array}{c}\n",
    "  \\widehat{e}_{1}\\\\\n",
    "  \\widehat{e}_{2}\\\\\n",
    "  \\vdots\\\\\n",
    "  \\widehat{e}_{n}\n",
    "  \\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  \\vdots\\\\\n",
    "  0\n",
    "  \\end{array}\\right]$ ，第一行运算说明了\n",
    "  $\\sum_{i=1}^{n}\\widehat{e}_{i}=0$。\"$\\heartsuit$\" 代表与我们计算无关的值。)\n",
    "  \n",
    " <!-- ``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We will produce a graph here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "consider replace this simulation example with a real data example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:example}\n",
    ":label: example1\n",
    "\n",
    "下面是一个简单的数值模拟案例，我们以此来说明最小二乘法估计量的特性。给定\n",
    "$\\left(x_{1i},x_{2i},x_{3i},e_{i}\\right)^{\\prime}\\sim N\\left(0_{4},I_{4}\\right)$ ， 因变量 $y_{i}$ 的生成式为\n",
    "\n",
    "$$\n",
    "y_{i}=0.5+2\\cdot x_{1i}-1\\cdot x_{2i}+e_{i}\n",
    "$$\n",
    "\n",
    "在不知道 $x_{3i}$ 是多余的情况下，我们将 $y_{i}$ 对\n",
    "$\\left(1,x_{1i},x_{2i},x_{3i}\\right)$ 进行回归。\n",
    "\n",
    "最后得到的参数估计值与真实值接近。当然，由于样本容量较小，答案不是十分准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr); set.seed(2022-6-15)\n",
    "n = 20 # sample size \n",
    "K = 4 # number of paramters\n",
    "b0 = as.matrix( c(0.5, 2, -1, 0) ) # the true coefficient\n",
    "X = cbind(1, matrix( rnorm(n * (K-1)), nrow = n ) ) # the regressor matrix \n",
    "e = rnorm(n) # the error term\n",
    "Y = X %*% b0 + e # generate the dependent variable\n",
    "bhat = solve(t(X) %*% X, t(X) %*% Y ) %>% as.vector() %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们继续用上面的数值例子来验证一些代数性质。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ehat = Y - X %*% bhat \n",
    "as.vector( t(X) %*% ehat ) %>% print()\n",
    "MX = diag(n) - X %*% solve( crossprod(X) ) %*% t(X)\n",
    "\n",
    "data.frame(e = e, ehat = ehat, MXY = MX%*%Y ) %>% head()\n",
    "cat(\"The mean of the residual is \", mean(ehat), \".\\n\")\n",
    "cat(\"The mean of the true error term is\", mean(e), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到，上例中 $e_i$ 是用来生成 $y_i$ 的扰动项，它和 $\\widehat{e}_i$ 不是一回事。$e_i$ 只是出现在我们的数值模拟当中，在实际回归中我们只观测到 $(y_i,x_i)$ ，而 $e_i$ 无从观测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几何意义\n",
    "\n",
    "$\\mathcal{X}=\\left\\{ Xb:b\\in\\mathbb{R}^{K}\\right\\}$ 是一个由 $X=\\left[X_{\\cdot1},\\ldots,X_{\\cdot K}\\right]$ 中的 $K$ 列生成的线性空间; 如果这些列是线性独立的，那么 $X$ 就是 $K$ 维的。最小二乘法估计量 $\\widehat \\beta$ 使得 $\\left\\Vert Y-Xb\\right\\Vert$ 最小化。换言之， $X\\widehat{\\beta}$ 是 $\\mathcal{X}$ 中与 $Y$ 最接近的那个点。\n",
    "\n",
    "\n",
    "等式 $Y=X\\widehat{\\beta}+\\widehat{e}$ 将 $Y$ 分解为两个垂直的分量， $X\\widehat{\\beta}$ 与 $\\widehat{e}$ ， 因为$\\left\\langle X\\widehat{\\beta},\\widehat{e}\\right\\rangle =\\widehat{\\beta}'X'\\widehat{e}=0_{K}^{\\prime}$ ， 其中$\\left\\langle \\cdot,\\cdot\\right\\rangle$ 是向量的内积运算。那么， $X\\widehat{\\beta}$ 就是 $Y$ 在 $\\mathcal{X}$ 上的 **投影** (projection)， $\\widehat{e}$ 则是对应的 **投影残差** (projection\n",
    "residuals)，根据勾股定理，自然有\n",
    "\n",
    "$$\n",
    "\\left\\Vert Y\\right\\Vert ^{2}=\\Vert X\\widehat{\\beta}\\Vert^{2}+ \\Vert \\widehat{e}\\Vert^{2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子向量 (Subvector) \n",
    "\n",
    "有时候，我们并不关心整个向量 $\\widehat{\\beta}$ 的数值，而是关心某个变量的系数，比如一个子向量 $\\widehat{\\beta}_1$ 的数值。**Frish-Waugh-Lovell (FWL) 定理** 揭示最小二乘法子向量的代数性质。要得到FWL定理，我们需要分块矩阵求逆的知识。对称实数正定矩阵\n",
    "$A=\\begin{pmatrix}A_{11} & A_{12}\\\\\n",
    "A_{12}' & A_{22}\n",
    "\\end{pmatrix}$\n",
    "逆矩阵可以写成\n",
    "\n",
    "$$\n",
    "A^{-1}=\\begin{pmatrix}\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & -\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1}A_{12}A_{22}^{-1}\\\\\n",
    "-A_{22}^{-1}A_{12}'\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & \\left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\\right)^{-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "将此性质运用至最小二乘法估计。记 $X=(X_1, X_2)$ ，则\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{pmatrix}\\widehat{\\beta}_{1}\\\\\n",
    "\\widehat{\\beta}_{2}\n",
    "\\end{pmatrix} & =\\widehat{\\beta}=(X'X)^{-1}X'Y\\\\\n",
    " & =\\left(\\begin{pmatrix}X_{1}'\\\\\n",
    "X_{2}'\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1} & X_{2}\\end{pmatrix}\\right)^{-1}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\\\\n",
    "X_{2}'X_{1} & X_{2}'X_{2}\n",
    "\\end{pmatrix}^{-1}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1} & -\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}\\\\\n",
    "\\heartsuit & \\heartsuit\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}.\n",
    "\\end{aligned}\n",
    "$$(eqn:fwl)\n",
    "\n",
    "$\\widehat{\\beta}$ 的子向量 $\\widehat{\\beta}_{1}$ 可写成 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\beta}_{1} & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}X_{2}'Y\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'P_{X_{2}}Y\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}\\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\\right)\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'M_{X_{2}}Y.\\end{aligned}\n",
    "$$\n",
    "\n",
    "以上 $\\widehat{\\beta}_{1}$ 的表达式就是 FWL 定理。该表达式意味着我们可以用三步方法得到 $\\widehat{\\beta}_{1}$ : \n",
    "\n",
    "1.将 $Y$ 对 $X_{2}$ 做回归，得到残差 $\\tilde{Y}$ ；\n",
    "\n",
    "2.将 $X_{1}$ 对 $X_{2}$ 做回归，得到残差 $\\tilde{X}_{1}$ ；\n",
    "\n",
    "3.将 $\\tilde{Y}$ 对 $\\tilde{X}_{1}$ 做回归，得到最小二乘法估计值 $\\widehat{\\beta}_{1}$ 。\n",
    "\n",
    "同样的方法也适用于总体线性投影 (population linear\n",
    "projection)，参考 Hansen (2020) \\[E\\] Chapter 2.22-23。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# verify FWL theorem\n",
    "\n",
    "X1 = X[,1:2];X2 = X[,3:4]\n",
    "PX2 = X2 %*% solve( t(X2) %*% X2) %*% t(X2) \n",
    "MX2 = diag(rep(1,n)) - PX2\n",
    "\n",
    "bhat1 <- (solve(t(X1)%*% MX2 %*% X1, t(X1) %*% MX2 %*% Y )) %>%\n",
    " as.vector() %>% print()\n",
    "\n",
    "ehat1 = MX2 %*% Y - MX2 %*% X1 %*% bhat1 \n",
    "data.frame(ehat = ehat, ehat1 = ehat1) %>% head() %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适配度 (Goodness of Fit) \n",
    "\n",
    "考虑一个带截距项的最小二乘法估计\n",
    "\n",
    "$$\n",
    "Y=\\widehat{Y}+\\widehat{e}=(X_{1}\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2})+\\widehat{e}\n",
    "$$(eqn:2)\n",
    "\n",
    "应用FWL定理，令 $X_{2}=\\iota$ ，其中希腊字母 $\\iota$ (iota)是一个所有位置都为1的 $n\\times1$ 向量。那么$M_{X_{2}}=M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota'$ 。其中， $M_{\\iota}z=z-\\bar{z}$ 。也就是说，我们在原本的向量 $z$ 中减去其均值 $\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$ 。\n",
    "\n",
    "那么，上述的三步方法变为\n",
    "\n",
    "1.将 $Y$ 对 $\\iota$ 做回归，得到残差 $M_{\\iota}Y$ ；\n",
    "\n",
    "2.将 $X_{1}$ 对 $\\iota$ 做回归，得到残差 $M_{\\iota}X_{1}$ ；\n",
    "\n",
    "3.将 $M_{\\iota}Y$ 对 $M_{\\iota}X_{1}$ 做回归，根据 {eq}`eqn:fwl` 得到最小二乘法估计值 $\\widehat{\\beta}_{1}$ 。\n",
    "\n",
    "\n",
    "我们将最后一步进行分解 \n",
    "\n",
    "$$\n",
    "M_{\\iota}Y=M_{\\iota}X_{1}\\widehat{\\beta}_{1}+\\tilde{e},\n",
    "$$(eqn:3)\n",
    "\n",
    "应用勾股定理得到\n",
    "\n",
    "$$\n",
    "\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}=\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}+ \\Vert \\widehat{e} \\Vert^{2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:exercise}\n",
    ":label: exercise31\n",
    "\n",
    "证明：{eq}`eqn:2` 式中的 $\\widehat{e}$ 与 {eq}`eqn:3` 式中的 $\\tilde{e}$ 相等。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中， **决定系数** ( $R^2$ ) 是一个衡量适配度的指标。样本内决定系数(in-sample $R^2$)可写作\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}=1-\\frac{\\left\\Vert \\tilde{e}\\right\\Vert ^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}.\n",
    "$$\n",
    "\n",
    "当归回包含截距项时， $R^2$ 才有定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:exercise}\n",
    ":label: exercise32\n",
    "\n",
    "在 {eq}`eqn:2` 式的分解下，证明 $R^{2}$ 即为 $\\widehat{Y}$ 的样本方差与 $Y$ 的样本方差之比：\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\widehat{Y}'M_{\\iota}\\widehat{Y}}{Y'M_{\\iota}Y}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y_{i}}-\\overline{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}}.\n",
    "$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决定系数 $R^2$ 的大小在不同的实际问题当中差别很大。$R^2$ 大于90%的情况在带有滞后效应的宏观模型中并不罕见。然而，在截面数据回归 (cross sectional regressions) 中， $R^2$ 的值经常是低于20%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:exercise}\n",
    ":label: exercise33\n",
    "\n",
    "考虑一个较“短”的回归 “将 $y_{i}$ 对 $x_{1i}$ 做回归” 与一个较“长”的回归\n",
    " “将 $y_{i}$ 对 $\\left(x_{1i},x_{2i}\\right)$ 做回归”: 给定相同的数据集 $\\left(Y,X_{1},X_{2}\\right)$ ， 试说明“短”回归的 $R^2$ 不比“长”回归的 $R^2$ 大。也就是说，通过增加更多的回归变我们总能使得 $R^2$ 不会变小。) \n",
    "```\n",
    "\n",
    "在传统意义上，回归因子的数量 $K$ 总是远小于样本量 $n$ 。然而，在大数据时代，回归变量的数量 $K$ 有可能大于样本量 $n$ 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:exercise}\n",
    ":label: exercise34\n",
    "\n",
    "证明: 当 $K\\geq n$ 时， $R^{2}=1$。注意，若 $K>n$ ， 则矩阵 $X'X$ 是缺秩 (rank deficient) 的。在这种情况下，我们可以将最小二乘法的定义扩展， $\\hat \\beta$ 仍然是使得 $\\left\\Vert Y-Xb\\right\\Vert ^{2}$ 最小化的参数值，但这个值不是唯一的。\n",
    "\n",
    "以下是本练习的数值实例。\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 5; K = 6; \n",
    "Y = rnorm(n)\n",
    "X = matrix( rnorm(n*K), n)\n",
    "summary( lm(Y~X) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个新的数据集 $\\left(Y^{\\mathrm{new}},X^{\\mathrm{new}}\\right)$ ， 样本外决定系数(OOS $R^2$) 可写作\n",
    "\n",
    "$$\n",
    "OOS\\ R^{2}=\\frac{\\widehat{\\beta}^{\\prime}X^{\\mathrm{new}\\prime}M_{\\iota}X^{\\mathrm{new}}\\widehat{\\beta}}{Y^{\\mathrm{new}\\prime}M_{\\iota}Y^{\\mathrm{new}}}.\n",
    "$$\n",
    "\n",
    "从原始数据集中得到参数估计值以后， $OOS\\ R^{2}$ 衡量这一参数估计值在新数据集上的适配度。在金融市场的短期预测模型中，如果某策略能够系统性地获得2\\%的 $OOS\\ R^{2}$ ，那么这就是一个很好的赚钱策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 总结\n",
    "\n",
    "本章完全是线性代数操作，无关数据背后的概率模型。只要 $X'X/n$ 可逆，以上线性代数性质对任何有限样本 $n$ 都成立。\n",
    "\n",
    "\n",
    "```{admonition} 历史趣闻\n",
    "Carl Friedrich Gauss (1777--1855) 宣称他在1795年就发明了最小二乘法，那时他用三个数据点预测了1801年矮行星谷神星的位置。虽然 Gauss 在1809年才发表了相关文章，而 Adrien-Marie Legendre (1752--1833) 在1805年就公开提出了此方法，但今天人们仍倾向于将最小二乘法的发明归功于 Gauss。因为像 Gauss 这样的数学巨人没有必要通过撒谎来窃取 Legendre 的成果。\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
